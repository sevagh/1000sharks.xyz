<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="GitLab Pages">
    <title>1000 sharks - an AI music experience</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body id="index">
    <div class="navbar">
      <a href="./mumt618_report.html">MUMT 618 report</a>
      <a href="./">1000sharks demo</a>
    </div>

    <h1>prism-samplernn experiments (3-tier)</h1>

    The RNCM (Royal Northern College of Music) PRiSM (Practice and Research in Science and Music) lab released a modern implementation of 3-tier SampleRNN<sup><a href="#ref">[1]</a></sup>:
    <blockquote>
    PRiSM is shortly going to publish its own implementation, using TensorFlow 2, and we’ll be explaining the features of the PRiSM SampleRNN in our next instalment – when we will also make the code available on PRiSM’s GitHub pages, along with a number of pretrained and optimised models.
    </blockquote>
    Since the dadabots discourage the use of their own 3-tier model, we'll use the PRiSM repo (where the 3-tier model is the primary focus).

    <h2>Python setup + minor code tweaks</h2>

    I forked the original prism-samplernn<sup><a href="#ref">[2]</a></sup> codebase to my own GitHub profile to make a minor adjustment in the scripts<sup><a href="#ref">[3]</a></sup>.
    <pre>
    physical_devices = tf.config.list_physical_devices('GPU')
    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)
    </pre>
    Without this parameter, the training would crash on my GPU (RTX 2070 SUPER) with a mysterious error message, "Fail to find the dnn implementation". It's an esoteric fix that one can find scattered across GitHub.
    <br>
    <br>
    The Python setup is straightforward using conda<sup><a href="#ref">[4]</a></sup> and following the project's README.md:
    <pre>
    $ conda create -n prism-samplernn python=3.8 anaconda
    $ conda activate prism-samplernn
    (prism-samplernn) $ pip install -r requirements.txt
    </pre>

    <h2>SampleRNN configuration</h2>
    I'll summarize the available SampleRNN hyperparameters and other customizeable steps compared across the original 2017 ICLR implementation, the Dadabots fork, the PRiSM fork which I use throughout the rest of this report, and finally my own modifications to the PRiSM parameters after experiment 0:
    <br>
    <br>
    <table>
      <tr>
        <th></th>
        <th>Original</th>
        <th>Dadabots</th>
        <th>PRiSM</th>
        <th>Mine</th>
        <th>Descr</th>
      </tr>
      <tr>
        <th>RNN layers</th>
        <td>4</td>
        <td>5</td>
        <td>4</td>
        <td>5</td>
        <td>Quality of results (dadabots note that 5 learns music better than 4)</td>
      </tr>
      <tr>
        <th>Tiers</th>
        <td>2 or 3</td>
        <td>2 or 3 (2 recommended for good music)</td>
        <td>3</td>
        <td>2, 3</td>
        <td>Tiers of RNN (more = wider temporal timescale, but...*)</td>
      </tr>
      <tr>
        <th>Frame sizes (corresponds to tiers)</th>
        <td>16</td>
        <td>16</td>
        <td>16,64</td>
        <td>3: 16,64, 2: 16</td>
        <td>Samples apart between low and high timescales</td>
      </tr>
      <tr>
        <th>Sample rate</th>
        <td>16000 (fixed)</td>
        <td>16000</td>
        <td>16000</td>
        <td>16000</td>
        <td>Sample rate of training/generating waveform (lower = faster learning, better able to learn long-timescale patterns)</td>
      </tr>
      <tr>
        <th>Training input</th>
        <td>No details</td>
        <td>Chop albums into 8s + 1s overlap</td>
        <td>Chop albums into 8s + 1s overlap</td>
        <td>Chop albums into 8s + 1s overlap</td>
        <td>Suggestions on how to prepare training data</td>
      </tr>
      <tr>
        <th>Epochs</th>
        <td>Not customizeable</td>
        <td>Not customizeable</td>
        <td>100</td>
        <td>100, 250</td>
        <td>Entire cycles of training on the same data (more = possibly better learning, but not necessarily)</td>
      </tr>
    </table> 
    <br>
    The frame sizes 16, 64 correspond to the additional tiers of SampleRNN (the first tier is always n = 1, or consecutive samples). A 2-tier SampleRNN architecture has a wider temporal scale of learning at 16 frames, while a 3-tier SampleRNN architecture learns at 16 and 64 frames.

    <h1>Preliminary experiments: training on synthetic waveforms</h1>
    Since SampleRNN operates on samples, it means I can use non-musical synthetic waveforms to verify some basic behaviors. <b>Disclaimer</b> that this is just an attempt to peek at SampleRNN's black box decisions, and not a rigorous experimental method. Using MATLAB, I generated some different sequences and permutations of floating point vectors:
    <pre>
    fs = 16000; % doesn't really matter with such fake numbers
    x1 = repelem(0.3, 4800000);
    x2 = [repelem(0.3, 5*16000) repelem(0.75, 5*16000) repelem(0.3, 5*16000) repelem(0.75, 5*16000)];
    x3 = [0.3 0.75];
    x3_big = repmat(x3, 1, 24000000);
    x4 = [0.3 0.66];
    x4_big = repmat(x4, 1, 24000000);
    audiowrite('0_3.wav', x1, fs);
    audiowrite('0_3_0_75_5050.wav', x2, fs);
    audiowrite('0_3_0_75_interleaved_samplewise.wav', x3_big, fs);
    audiowrite('0_3_0_66_interleaved_samplewise.wav', x4_big, fs);
    </pre>
    The above commands create a variety of "fake" wav files containing some simple sequences of floats, which I then used as training inputs for SampleRNN.
    <h2>Training to fit a sequence of all 0.3</h2>
    The first thing I verified is whether we can simply train SampleRNN only on samples <code>[0.3, 0.3, ...]</code> and have it be 100% overfitted (and generate exactly only <code>[0.3, 0.3, ...]</code> as a result).
    The training command (which outputs an example generated clip that I used as <code>y[n]</code>) is as follows. I mimicked the real preprocessing and training steps:
    <pre>
    # chop up the fake wav clip of [0.3, 0.3, ...] into 8-second overlapping clips
    $ python ./chunk_audio.py --input_file ./0_3.wav --output_dir ./testdata-waveform-0.3/ --chunk_length 8000 --overlap 1000

    # train on the folder of [0.3, 0.3, ...]
    $ python train.py --id testdata-waveform-0.3 --data_dir ./testdata-waveform-0.3 --num_epochs 5 --batch_size 64 --sample_rate 16000

    # the displayed y[n] is emitted during training in "generated/testdata-waveform-0.3/testdata-waveform-0.3_epoch_1.wav"
    </pre>
    Within 2 epochs (~15 minutes of training), the loss was 0.0% and accuracy was 100.0%, indicating that the model learned quickly how to generate the value 0.3:
    <img src="prelim_0.3_overfit.png" width=800px/>
    Interestingly, the values of the generated waveform aren't exactly 0.3 but 0.306 - I don't know where that comes from but it could be from the round-trip through SampleRNN's quantization layers.

    <h2>Training on a mixture of 0.3 and 0.75</h2>
    My next test was to see what happened if I trained on a mixture of 0.3 and 0.75. Recall the creation command:
    <pre>
    x2 = [repelem(0.3, 5*16000) repelem(0.75, 5*16000) repelem(0.3, 5*16000) repelem(0.75, 5*16000)];
    </pre>
    At a sampling rate of 16000 Hz, this is 5 seconds of 0.3 followed by 5 seconds of 0.75 (repeated 2x overall). In addition to the clip emitted during training, I also generated an additional 4 clips. The results were as follows (<b>n.b.</b> this is a dense stem plot that makes it look like colored rectangles):
    <img src="prelim_0.3_0.75.png" width=800px/>
    Every generated clip had the value 0.306, and not 0.75. The model only learned how to generate 0.3s from observing a 50/50 split of monolithic sequences of <code>[0.3, ...]</code> and <code>[0.75, ...]</code>, which is an interesting outcome. I generated 10s of clips that all only contained 0.3.

    <h2>Training on interleaved 0.3,0.75 consecutive sample pairs</h2>
    The lowest and most important temporal scale of learning of SampleRNN is on the level of individual consecutive samples. An easy experiment to verify this is to create a sequence of <code>[0.3, 0.75, 0.3, 0.75, ...]</code> and validate that the output is similar:
    <img src="prelim_0.3_0.75_interleave.png" width=800px/>
    The output is the same, which is expected.

    <h2>Training on mixed interleaved 0.3,0.75 and 0.3,0.66 consecutive sample pairs</h2>
    In a similar experiment to mixing 0.3 and 0.75 (where the outcome was that only 0.3 was generated as an output), let's train on a 50/50 mix of interleaved pairs of 0.3,0.75 and 0.3,0.66:
    <img src="prelim_0.3_0.66_0.75_interleave.png" width=800px/>
    As we saw with the 50/50 mix of 0.3 and 0.75, the output seems to adhere to only one of the training inputs, and not both.

    <h2>Training with 16-sample temporal patterns</h2>
    Next up is to test the "tiers = temporal scales of learning" hypothesis. To recap, in the 3-tier SampleRNN, there should be learning at the level of consecutive samples, followed by learning on broader frames that are of size 16 and 64 samples. Let's create another fake waveform with a ramp that repeats at every 16 samples:
    <pre>
    x = linspace(0.1, 0.9, 16);
    x_big = repmat(x, 1, 2400000);
    audiowrite('frame_16_pattern.wav', x_big, 16000);
    </pre>
    Note that while this is very different from a real pattern in music, it should still count as a pattern in the waveform that SampleRNN can learn:
    <img src="prelim_ramp_16.png" width=800px/>
    This is one of my favorite results of the preliminary experiments, showing that SampleRNN could faithfully recreate the ramping pattern, but with some oddities (negative-valued samples).
    The accuracy and loss capped at the following values after 1 epoch:
    <pre>
    Epoch: 5/5, Step: 500/625, Loss: 1.386, Accuracy: 75.000, (0.445 sec/step)
    Epoch: 5/5, Step: 501/625, Loss: 1.386, Accuracy: 75.000, (0.429 sec/step)
    Epoch: 5/5, Step: 502/625, Loss: 1.386, Accuracy: 75.000, (0.426 sec/step)
    Epoch: 5/5, Step: 503/625, Loss: 1.386, Accuracy: 75.000, (0.427 sec/step)
    Epoch: 5/5, Step: 504/625, Loss: 1.386, Accuracy: 75.000, (0.425 sec/step)
    Epoch: 5/5, Step: 505/625, Loss: 1.386, Accuracy: 75.000, (0.426 sec/step)
    Epoch: 5/5, Step: 506/625, Loss: 1.386, Accuracy: 75.000, (0.428 sec/step)
    </pre>

    <h2>Training with 32-sample temporal patterns</h2>
    Let's repeat the above but with 32 samples. Since the frame sizes of learning are 16 and 64, what could the outcome of 32 be?
    <pre>
    x = linspace(0.1, 0.9, 32);
    x_big = repmat(x, 1, 1200000);
    audiowrite('frame_32_pattern.wav', x_big, 16000);
    </pre>
    An interesting note is that during the training I immediately noticed that even from the first epoch of training on 32-sample temporal patterns, the loss was lower and accuracy higher:
    <pre>
    Loading corpus entry ./testdata-waveform-32frame/frame_32_pattern_chunk_97.wav
    Loading corpus entry ./testdata-waveform-32frame/frame_32_pattern_chunk_50.wav
    Epoch: 1/5, Step: 250/625, Loss: 1.026, Accuracy: 82.249, (0.466 sec/step)
    Epoch: 1/5, Step: 251/625, Loss: 1.026, Accuracy: 82.257, (0.438 sec/step)
    Epoch: 1/5, Step: 252/625, Loss: 1.025, Accuracy: 82.266, (0.438 sec/step)
    Epoch: 1/5, Step: 253/625, Loss: 1.024, Accuracy: 82.274, (0.432 sec/step)
    Epoch: 1/5, Step: 254/625, Loss: 1.024, Accuracy: 82.282, (0.440 sec/step)
    Epoch: 1/5, Step: 255/625, Loss: 1.023, Accuracy: 82.290, (0.433 sec/step)
    Epoch: 1/5, Step: 256/625, Loss: 1.023, Accuracy: 82.299, (0.458 sec/step)
    Epoch: 1/5, Step: 257/625, Loss: 1.022, Accuracy: 82.307, (0.455 sec/step)
    Epoch: 1/5, Step: 258/625, Loss: 1.021, Accuracy: 82.315, (0.460 sec/step)
    </pre>
    It capped within 2 epochs at a higher value than the previous:
    <pre>
    Generated sample output to ./generated/testdata-waveform-32frame/testdata-waveform-32frame_epoch_1.wav
    Done
    Epoch: 2/5, Step: 1/625, Loss: 0.868, Accuracy: 84.375, (0.438 sec/step)
    Epoch: 2/5, Step: 2/625, Loss: 0.867, Accuracy: 84.375, (0.472 sec/step)
    Epoch: 2/5, Step: 3/625, Loss: 0.867, Accuracy: 84.375, (0.471 sec/step)
    </pre>
    This indicates that 32-sample temporal patterns are easier for the model to converge to than 16-samples. Here's the result generated waveform:
    <img src="prelim_ramp_32.png" width=800px/>

    <h2>Training with two "tiers" of temporal patterns</h2>
    Let's try a more complicated pattern - ramps that are 50 samples long, but with a larger pattern of "ramping ramps":
    <pre>
    x_1 = linspace(0.1, 0.2, 50);
    x_2 = linspace(0.1, 0.4, 50);
    x_3 = linspace(0.1, 0.6, 50);
    x_4 = linspace(0.1, 0.8, 50);

    x_big = repmat([x_1 x_2 x_3 x_4], 1, 100000);
    audiowrite('frame_double_pattern.wav', x_big, 16000);
    </pre>
    Loss and accuracy cap within 2 epochs:
    <pre>
    Loading corpus entry ./testdata-waveform-double-pattern/frame_double_pattern_chunk_310.wav
    Loading corpus entry ./testdata-waveform-double-pattern/frame_double_pattern_chunk_226.wav
    Loading corpus entry ./testdata-waveform-double-pattern/frame_double_pattern_chunk_175.wav
    Epoch: 2/5, Step: 1/625, Loss: 0.103, Accuracy: 98.438, (5.912 sec/step)
    Epoch: 2/5, Step: 2/625, Loss: 0.092, Accuracy: 98.486, (0.425 sec/step)
    Epoch: 2/5, Step: 3/625, Loss: 0.089, Accuracy: 98.503, (0.422 sec/step)
    Epoch: 2/5, Step: 4/625, Loss: 0.087, Accuracy: 98.511, (0.427 sec/step)
    </pre>
    Results:
    <img src="prelim_ramp_of_ramps.png" width=800px/>
    It looks to me as if SampleRNN overfitted (desired, in this case) very well to the ramp-of-ramps pattern that spanned multiple temporal timescales.

    <h2>Preliminary tests - thoughts</h2>
    We can see that 3-tier SampleRNN generalizes to several different waveform patterns, from simple 16-sample ramps to a 200-sample meta-pattern (ramp of ramps).
    <br>
    <br>
    One unexplained result is why, when presented with a 50/50 mix of different outcomes, SampleRNN adheres to only one of the training datasets and doesn't generate two different results. In the 50/50 tests, I would have expected at least 1 generated clip to only consist of <code>[0.75, ...]</code>.

    <h1>Experiment 0: training on a single album</h1>
    We'll train 3-tier SampleRNN first on Animals as Leaders' self-titled album:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/SZ2WrN93vno" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    I downloaded the audio using youtube-dl, converted it to 16kHz mono with ffmpeg (recommended for SampleRNN to perform better), split it up into chunks (ignoring silence) using the prism-samplernn script <code>chunk_audio.py</code>, and ran the training with default parameters:
    <pre>
    $ python train.py --id aamgen --data_dir ./chunks/ --num_epochs 100 --batch_size 64 --checkpoint_every 5 --output_file_dur 3 --sample_rate 16000
    </pre>
    This emitted some generated clips during the training. Let's listen to 2 of the more musically interesting clips (a lot of them are just silence), generated at epoch 20 and 85 of the 100-epoch training. An epoch is one cycle of the entire training dataset - this means that the neural network observed the same album 100 times iteratively to learn how to model it:
    <br>
    Epoch 20:<br>
    <audio controls>
	    <source src="aamgen_epoch_20.wav" type="audio/wav"</source>
    </audio>
    <br>
    Epoch 85:<br>
    <audio controls>
	    <source src="aamgen_epoch_85.wav" type="audio/wav"</source>
    </audio>
    <br>
    <br>
    After the training was done (it took ~3 days on my machine), I generated 2 10-second clips of what I thought would be "Animas-as-Leaders-esque" music. The generate command is:
    <pre>
    $ python generate.py --output_path ./aamgen-out/aamgen.wav --checkpoint_path ./logdir/aamgen/14.09.2020_19.02.06/model.ckpt-90 --config_file ./default.config.json --num_seqs 2 --dur 10 --sample_rate 16000
    </pre>
    This says to use the model checkpoint 90. Even though we specified 100 epochs in the training, the model has an intelligent stop when it sees that additional training is not improving the model. In this case, it seems like 90 epochs exhausted the model's learning ability. Here's one of them (both sound equally bad):
    <br>
    <audio controls>
	    <source src="aamgen_0.wav" type="audio/wav"</source>
    </audio>

    <h2>Experiment 0 lessons</h2>
    I applied tweaks that Karl Hiner<sup><a href="#ref">[5]</a></sup> did in his experiments - for my next experiment, I would try some or all of the following:
    <ul>
	    <li>Increase the number of RNN layers from 4 to 5 to try to create more cohesive music</li>
	    <li>Use more training data than just one album</li>
	    <li>Increase epochs from 100 to 250 (longer training may lead to better results)</li>
    </ul>
    We're also probably running into the same discovery of others, that 3-tier architecture of the PRiSM-SampleRNN implementation may be producing worse music than 2-tier.

    <h1>Experiment 1: longer training on multiple albums</h1>
    For my next experiment, I downloaded instrumental versions of the albums of Periphery (instrumental - I didn't want vocals mixing into the results, as I want to focus on musical instruments acoustics) and Mestis (an instrumental band). The data fetch and preprocessing scripts are available in my prism-samplernn fork:
    <pre>
    #!/usr/bin/env bash
    
    echo "Fetching training data - youtube-dl wav files for Mestis and Periphery albums"
    
    # youtube playlists for Mestis - Eikasia, Polysemy, Basal Ganglia
    mestis_album_1="PLNOrZEIoYAMgLJeZeCUEhABLPz7yqkyfI"
    mestis_album_2="PLfoVvOUi1CqV0O-yMdOvTff_vp8hOQnWi"
    mestis_album_3="PLRK89uMjq03BMsxBKFGBcDAh2G7ACwJMK"
    
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${mestis_album_1}
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${mestis_album_2}
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${mestis_album_3}
    
    # youtube playlists for instrumental Periphery albums - Periphery III, I, II, IV, Omega, Juggernaut
    periphery_album_1="PLSTnbYVfZR03JGmoJri6Sgvl4f0VAi9st"
    periphery_album_2="PL7DVODcLLjFplM5Rw-bNUyrwAECIPRK26"
    periphery_album_3="PLuEYu7jyZXdde7ePWV1RUvrpDKB8Gr6ex"
    periphery_album_45="PLEFyfJZV-vtKeBedXTv82yxS7gRZkzfWr"
    periphery_album_6="PL6FJ2Ri6gSpOWcbdq--P5J0IRcgH-4RVm"
    
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_1}
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_2}
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_3}
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_45}
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_6}
    
    mkdir -p periphery-raw
    mkdir -p mestis-raw
    
    find . -maxdepth 1 -mindepth 1 -type f -iname '*PERIPHERY*.wav' -exec mv {} periphery-raw/ \;
    find . -maxdepth 1 -mindepth 1 -type f -iname '*MESTIS*.wav' -exec mv {} mestis-raw/ \;
    find . -maxdepth 1 -mindepth 1 -type f -iname '*Javier*.wav' -exec mv {} mestis-raw/ \;
    find . -maxdepth 1 -mindepth 1 -type f -iname '*Suspiro*.wav' -exec mv {} mestis-raw/ \;
    find . -maxdepth 1 -mindepth 1 -type f -name '*.wav' -exec rm {} \;
    
    mkdir -p mestis-processed
    mkdir -p periphery-processed
    
    echo "Processing each wav file to 16kHz mono"
    
    for f in mestis-raw/*.wav; do
            ffmpeg -i "${f}" -ac 1 -ar 16000 "mestis-processed/$(basename "$f")";
    done
    
    for f in periphery-raw/*.wav; do
            ffmpeg -i "${f}" -ac 1 -ar 16000 "periphery-processed/$(basename "$f")";
    done
    
    mkdir -p periphery-chunks
    mkdir -p mestis-chunks
    mkdir -p mixed-chunks
    
    for f in mestis-processed/*.wav; do
            python ../chunk_audio.py --input_file "${f}" --output_dir mestis-chunks --chunk_length 8000 --overlap 1000
            python ../chunk_audio.py --input_file "${f}" --output_dir mixed-chunks --chunk_length 8000 --overlap 1000
    done
    
    for f in periphery-processed/*.wav; do
            python ../chunk_audio.py --input_file "${f}" --output_dir periphery-chunks --chunk_length 8000 --overlap 1000
            python ../chunk_audio.py --input_file "${f}" --output_dir mixed-chunks --chunk_length 8000 --overlap 1000
    done
    </pre>
    What the script does is:
    <ul>
	    <li>Fetch files for every Mestis song (from YouTube playlists + youtube-dl)</li>
	    <li>Fetch files for every instrumental Periphery song (from YouTube playlists + youtube-dl)</li>
	    <li>Pre-process them into 16kHz mono with ffmpeg (for optimal training)</li>
	    <li>Apply the chunk_audio.py script to split into non-silent 8-second chunks with 1 second overlap</li>
	    <li>Create 3 sets of training data - periphery-chunks, mestis-chunks, mixed-chunks</li>
    </ul>
    My intention was to train the model on each of the sets of training chunks, to create generated music that:
    <ul>
	    <li>Sounds like Periphery only</li>
	    <li>Sounds like Mestis only</li>
	    <li>Sounds like "Mestiphery", an organic mashup</li>
    </ul>
    <h2>Experiment 1 part 1 - Periphery only</h2>
    The training command for periphery-only is:
    <pre>
$ python train.py --id periphery_only --data_dir ./experiment-1/periphery-chunks/ --num_epochs 100 --batch_size 64 --sample_rate 16000
    </pre>
    <h3>Periphery only results</h3>
    Here's a 30-second clip output from the training on Periphery only:
    <br>
    <audio controls>
	    <source src="periphery_only_0.wav" type="audio/wav"</source>
    </audio>
    <br>
    <br>
    Another trait is that most generated audio consists of silence. I was very lucky to get almost 30 seconds of musical content in a single clip. Subjectively, this sounds nothing like Periphery:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/1_VZji3YFo4?start=90" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    Some more clips show the high-pitched output (which is melodic, but again, seemingly bizarre when trained on downtuned palm-muted rhythm guitar riffs).
    <br>
    Epoch 67:
    <br>
    <audio controls>
	    <source src="periphery_only_epoch_67.wav" type="audio/wav"</source>
    </audio>
    <br>
    Epoch 70:
    <br>
    <audio controls>
	    <source src="periphery_only_epoch_70.wav" type="audio/wav"</source>
    </audio>
    <br>
    <h4>Overfitting to a single song, Make Total Destroy</h4>
    Since SampleRNN was trained on 6 different albums, let's narrow down why it wasn't able to create Periphery's characteristic sound by overfitting specifically on the song shown above.
    <br>
    <br>
    After creating a test dataset with only the song Make Total Destroy and training on it for 100 epochs, the model reaches the following loss and accuracy:
    <pre>
    Epoch: 91/100, Step: 118/125, Loss: 0.867, Accuracy: 74.538, (0.246 sec/step)
    Epoch: 91/100, Step: 119/125, Loss: 0.864, Accuracy: 74.616, (0.255 sec/step)
    Epoch: 91/100, Step: 120/125, Loss: 0.861, Accuracy: 74.705, (0.247 sec/step)
    Epoch: 91/100, Step: 121/125, Loss: 0.857, Accuracy: 74.797, (0.246 sec/step)
    Epoch: 91/100, Step: 122/125, Loss: 0.854, Accuracy: 74.893, (0.246 sec/step)
    Epoch: 91/100, Step: 123/125, Loss: 0.850, Accuracy: 74.985, (0.251 sec/step)
    Epoch: 91/100, Step: 124/125, Loss: 0.847, Accuracy: 75.076, (0.246 sec/step)
    </pre>
    Generated audio from epoch 91:
    <br>
    Clip 1:
    <br>
    <audio controls>
	    <source src="maketotaldestroy_1.wav" type="audio/wav"</source>
    </audio>
    <br>
    Clip 2:
    <br>
    <audio controls>
	    <source src="maketotaldestroy_2.wav" type="audio/wav"</source>
    </audio>
    <br>
    These don't sound anything like music.

    <h3>Generated audio visualization</h3>
    The above clips show collections of realistic note onsets. One of my original statements about WaveNet and SampleRNN was that they could produce music with convincing dynamics and timbre, to make us believe real humans played it. Observing various aspects of the waveform in the time and frequency domain should be useful.
    <br>
    <br>
    Let's view each clip (epochs 67, 70) in the time domain and frequency domain (with a spectrogram):
    <img src="periphery_epoch_67_timedomain.png" width=800px/>
    <img src="periphery_epoch_70_timedomain.png" width=800px/>
    <img src="periphery_epoch_67_spectrogram.png" width=800px/>
    <img src="periphery_epoch_70_spectrogram.png" width=800px/>
    <br>
    Although this is subjective, one can see the dynamic nature of the produced audio in the plots above. It really does look like there are real musical variations in the complex waveform (aside from the totally blank silences which are odd in real music).

    <h3>Mu-law vs linear quantization</h3>
    Karl Hiner's blog post touches on WaveNet's mu-law quantization, and claims it sounds better than SampleRNN's linear quantization. In fact, I found that every SampleRNN implementation I found had options for linear and mu-law quantization (perhaps it was added later). In fact the original ICLR 2017 paper even has an "a-law quantization" (similar to mu-law). Let's hear what each sounds like:
    <br>
    <br>
    Periphery epoch 83, mu-law quantization:
    <br>
    <audio controls>
	    <source src="periphery_epoch_83_mulaw.wav" type="audio/wav"</source>
    </audio>
    <br>
    Periphery epoch 83, linear quantization <b>WARNING! LOUD!</b>:
    <br>
    <audio controls>
	    <source src="periphery_epoch_83_linear.wav" type="audio/wav"</source>
    </audio>
    <br>
    In my subjective listening test, the linear quantization output is very loud, almost to the point of distortion and clipping. The mu-law quantization is outputting music with more subtle volume, possibly since its more suited for the logarithmic human experience of loudness.

    <h2>Abandoning the original hypotheses</h2>
    The results were very different from my expectations:
    <ul>
      <li>The resulting generated audio is mostly silence and junk</li>
      <li>There are some interesting potentially musical sounds, but it's all high-pitched whistling and doesn't contain any characteristics of the band Periphery (palm-muted distorted guitar chords, etc.)</li>
      <li>I have to generate 100s or 1000s of clips and curate the results to create a final result - the chances of getting 1 cohesive "song" (let's say, 2 minutes of contiguous music) are pretty slim</li>
    </ul>
    With the demonstrated poor quality of results (e.g. generated audio that sounds nothing like the band), my original hypotheses were debunked. I mixed the Mestis data into the Periphery data, and continued training the model (that was initially trained only on Periphery for 100 epochs) for 250 epochs. At this point my goal was to "embrace the weird" and see what sort of strange music I can create. The sum total of all my training and experiments (including failed starts) for 3-tier SampleRNN was <b>3 weeks</b>.
    <h2>Training results</h2>
    The results of training SampleRNN are stored in the <code>logdir</code> directory, in timestamped directories storing checkpoints for intermediate epochs in training. Here's a <code>tree</code> view of of the training. The size of the training directory is 51GB, after having been trained on all of the albums listed above (709MB of music):
    <pre>
    $ tree logdir/periphery_only/ -L 2
    logdir/periphery_only/
    ├── 15.09.2020_17.36.46
    │   ├── checkpoint
    │   ├── model.ckpt-79.data-00000-of-00001
    │   ├── model.ckpt-79.index
    │   ├── model.ckpt-80.data-00000-of-00001
    │   ├── model.ckpt-80.index
    │   ├── model.ckpt-81.data-00000-of-00001
    │   ├── model.ckpt-81.index
    │   ├── model.ckpt-82.data-00000-of-00001
    │   ├── model.ckpt-82.index
    │   ├── model.ckpt-83.data-00000-of-00001
    │   ├── model.ckpt-83.index
    │   └── train
    ├── 17.09.2020_21.55.43
    │   ├── checkpoint
    │   ├── model.ckpt-89.data-00000-of-00001
    │   ├── model.ckpt-89.index
    │   ├── model.ckpt-90.data-00000-of-00001
    │   ├── model.ckpt-90.index
    │   ├── model.ckpt-91.data-00000-of-00001
    │   ├── model.ckpt-91.index
    │   ├── model.ckpt-92.data-00000-of-00001
    │   ├── model.ckpt-92.index
    │   ├── model.ckpt-93.data-00000-of-00001
    │   ├── model.ckpt-93.index
    │   └── train
    ...truncated...
    └── 22.09.2020_22.25.13
        ├── checkpoint
        ├── model.ckpt-244.data-00000-of-00001
        ├── model.ckpt-244.index
        ├── model.ckpt-245.data-00000-of-00001
        ├── model.ckpt-245.index
        ├── model.ckpt-246.data-00000-of-00001
        ├── model.ckpt-246.index
        ├── model.ckpt-247.data-00000-of-00001
        ├── model.ckpt-247.index
        ├── model.ckpt-248.data-00000-of-00001
        ├── model.ckpt-248.index
        └── train
    </pre>

    <h1>Repeating experiment 0 with 2-tier architecture</h1>
    I repeated Experiment 0, training on a single album (Animals as Leaders' self-titled album), but after modifying the prism-samplernn code to support the 2-tier architecture (which as discussed in the overview is purported to produce better music).
    <br>
    <br>
    The parameter `frame_sizes = [16,64]` determines the additional 2 tiers, frame and big frame (in addition to the base sample tier) - in 2-tier SampleRNN, there is no big frame. I modified my fork of prism-samplernn to accept `frame_size = [16]` as a configuration for 2-tier. The code changes can be viewed <a href="https://github.com/rncm-prism/prism-samplernn/compare/master...sevagh:master">here</a>. The exact steps of experiment 0 were repeated.
    <br>
    <br>
    Train and generate:
    <pre>
    $ python train.py --id aamgen-2t --data_dir ./chunks/ --num_epochs 100 --batch_size 64 --checkpoint_every 5 --output_file_dur 3 --sample_rate 16000 --resume=True
    Epoch: 98/100, Step: 735/750, Loss: 3.078, Accuracy: 21.368, (0.391 sec/step)
    Epoch: 98/100, Step: 736/750, Loss: 3.077, Accuracy: 21.386, (0.391 sec/step)
    Epoch: 98/100, Step: 737/750, Loss: 3.076, Accuracy: 21.406, (0.385 sec/step)
    Epoch: 98/100, Step: 738/750, Loss: 3.075, Accuracy: 21.428, (0.394 sec/step)
    Epoch: 98/100, Step: 739/750, Loss: 3.074, Accuracy: 21.451, (0.388 sec/step)
    Epoch: 98/100, Step: 740/750, Loss: 3.073, Accuracy: 21.475, (0.390 sec/step)
    Epoch: 98/100, Step: 741/750, Loss: 3.072, Accuracy: 21.501, (0.400 sec/step)
    Epoch: 98/100, Step: 742/750, Loss: 3.071, Accuracy: 21.529, (0.389 sec/step)
    Epoch: 98/100, Step: 743/750, Loss: 3.069, Accuracy: 21.559, (0.394 sec/step)
    Epoch: 98/100, Step: 744/750, Loss: 3.068, Accuracy: 21.590, (0.388 sec/step)
    Epoch: 98/100, Step: 745/750, Loss: 3.066, Accuracy: 21.623, (0.386 sec/step)
    Epoch: 98/100, Step: 746/750, Loss: 3.065, Accuracy: 21.656, (0.396 sec/step)
    Epoch: 98/100, Step: 747/750, Loss: 3.063, Accuracy: 21.691, (0.389 sec/step)
    Epoch: 98/100, Step: 748/750, Loss: 3.062, Accuracy: 21.726, (0.388 sec/step)
    Epoch: 98/100, Step: 749/750, Loss: 3.060, Accuracy: 21.764, (0.392 sec/step)
    $
    $ python generate.py --output_path ./aamgen-2t.wav --checkpoint_path ./logdir/aamgen-2t/31.10.2020_17.33.41/model.ckpt-95 --config_file ./default.config.json --num_seqs 20 --dur 20 --sample_rate 16000
    </pre>
    Generated clips:<br>
    <audio controls>
	    <source src="aamgen_2t_0.wav" type="audio/wav"</source>
    </audio>
    <br>
    <audio controls>
	    <source src="aamgen_2t_1.wav" type="audio/wav"</source>
    </audio>
    <br>
    <br>
    The resultant clips exhibit the same strange high-pitched whistling and erratic drum beats - no significant improvement. Although the code "works" (in that it doesn't crash and actually successfully trains and generates anything at all), I can't really say whether it "works" in the neural perspective, i.e. converges to a correct solution.

    <h1 id="ref">References</h1>
    <ol>
      <li><a href="https://www.rncm.ac.uk/research/research-centres-rncm/prism/prism-blog/a-short-history-of-neural-synthesis/">A Short History of Neural Synthesis - Royal Northern College of Music</a></li>
      <li><a href="https://github.com/rncm-prism/prism-samplernn">rncm-prism/prism-samplernn - GitHub</a></li>
      <li><a href="https://github.com/sevagh/prism-samplernn">rncm-prism/prism-samplernn - my fork on GitHub</a></li>
      <li><a href="https://docs.conda.io/en/latest/">Conda - Conda Documentation</a></li>
      <li><a href="https://karlhiner.com/music_generation/wavenet_and_samplernn/">Karl Hiner - Generating Music with WaveNet and SampleRNN</a></li>
    </ol>
  </body>
</html>

