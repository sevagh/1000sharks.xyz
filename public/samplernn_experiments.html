<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="GitLab Pages">
    <title>1000 sharks - an AI music experience</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body id="index">
    <div class="navbar">
      <a href="./">MUMT 618 report</a>
      <a href="./demo.html">1000sharks demo</a>
    </div>

    <h1>SampleRNN experiments</h1>

    <h2>Python setup + minor code tweaks</h2>

    I forked the prism-samplernn codebase to my own GitHub profile to make a minor adjustment in the scripts<sup><a href="#ref">[12]</a></sup>.
    <pre>
    physical_devices = tf.config.list_physical_devices('GPU')
    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)
    </pre>
    Without this parameter, the training would crash on my GPU (RTX 2070 SUPER) with a mysterious error message, "Fail to find the dnn implementation". It's an esoteric fix that one can find scattered across GitHub<sup><a href="#ref">[13]</a></sup>.
    <br>
    <br>
    The Python setup is straightforward using conda<sup><a href="#ref">[14]</a></sup> and following the project's README.md:
    <pre>
    $ conda create -n prism-samplernn python=3.8 anaconda
    $ conda activate prism-samplernn
    (prism-samplernn) $ pip install -r requirements.txt
    </pre>

    <h1>2-tier vs. 3-tier SampleRNN</h1>
    Something to note is that various sources mention that 2-tier SampleRNN creates better music than 3-tier SampleRNN:
    <ul>
	    <li> Dadabots<sup><a href="#ref">https://github.com/Cortexelus/dadabots_sampleRNN#training-samplernn-3-tier</a></sup>:
		    <blockquote>There's also a 3-tier option, but we initially had better results with 2-tier, so we don't use 3-tier. It doesn't have the modifications we made to 2-tier.</blockquote>
	    </li>
	    <li> SampleRNN paper reviewer comments<sup><a href="#ref">https://openreview.net/forum?id=SkxKPDv5xl</a></sup>:
		    <blockquote>Table 1 and Figure 4 also show the 2-tier SampleRNN outperforming the 3-tier model in terms of likelihood and human rating respectively, which is very counterintuitive as one would expect longer-range temporal correlations to be even more relevant for music than for speech. This is not discussed at all, I think it would be useful to comment on why this could be happening.</blockquote>
		 Author's reply:
		 <blockquote>
			 "Why 2-tier is outperforming the 3-tier model for music?"
			 - We did not expect that, but for any dataset and architecture structure, there is an optimal depth. Considering that this is a deep RNN (which introduces a form of recurrent depth, here very large) and the hypothesis that it is difficult to train such architectures in the first place, it is possible that alternative training procedures could yield better results with a deeper model.
		 </blockquote>
	    </li>
    </ul>
    <br>
    Recall that in the SampleRNN paper, the multiple tiers of the RNN determined the learning of audio patterns at different temporal scales. This is reflected in the following diagram from the paper:
    <figure>
    <img src="8_samplernn.png" width=700px/>
    <figcaption>Higher RNN tiers map to wider temporal scales <sup><a href="#ref">[3]</a></sup></figcaption>
    </figure>
    <br>
    The lowest temporal scale (consecutive samples) represents very low-level audio features (e.g. timbre), while higher scales can (hypothetically) go as far as representing repeating choruses or verses minutes apart. As such, it's interesting to note that 2-tiers, or only two temporal scales of learning, performed better than 3-tier, which should hypothetically be enforcing even longer-scale temporal patterns (and music has temporal patterns as coarse as minutes apart, e.g. a repeating chorus).
    <br>
    <br>
    However, as the author says, 2-tier SampleRNN may have a depth that makes it more optimal considering the training architecture of SampleRNN (or in other words, there needs to be an analysis of alternative training architectures to make 3-tier beat 2-tier).

    <h2>SampleRNN configuration</h2>
    I'll summarize the available SampleRNN hyperparameters and other customizeable steps compared across the original 2017 ICLR implementation, the Dadabots fork, the PRiSM fork which I use throughout the rest of this report, and finally my own modifications to the PRiSM parameters after experiment 0:
    <br>
    <br>
    <table>
      <tr>
        <th></th>
        <th>Original</th>
        <th>Dadabots</th>
        <th>PRiSM</th>
        <th>Mine</th>
        <th>Descr</th>
      </tr>
      <tr>
        <th>RNN layers</th>
        <td>4</td>
        <td>5</td>
        <td>4</td>
        <td>5</td>
        <td>Quality of results (dadabots note that 5 learns music better than 4)</td>
      </tr>
      <tr>
        <th>Tiers</th>
        <td>2 or 3</td>
        <td>2 or 3 (2 recommended for good music)</td>
        <td>3</td>
        <td>2, 3</td>
        <td>Tiers of RNN (more = wider temporal timescale, but...*)</td>
      </tr>
      <tr>
        <th>Frame sizes (corresponds to tiers)</th>
        <td>16</td>
        <td>16</td>
        <td>16,64</td>
        <td>3: 16,64, 2: 16</td>
        <td>Samples apart between low and high timescales</td>
      </tr>
      <tr>
        <th>Sample rate</th>
        <td>16000 (fixed)</td>
        <td>16000</td>
        <td>16000</td>
        <td>16000</td>
        <td>Sample rate of training/generating waveform (lower = faster learning, better able to learn long-timescale patterns)</td>
      </tr>
      <tr>
        <th>Training input</th>
        <td>No details</td>
        <td>Chop albums into 8s + 1s overlap</td>
        <td>Chop albums into 8s + 1s overlap</td>
        <td>Chop albums into 8s + 1s overlap</td>
        <td>Suggestions on how to prepare training data</td>
      </tr>
      <tr>
        <th>Epochs</th>
        <td>Not customizeable</td>
        <td>Not customizeable</td>
        <td>100</td>
        <td>100, 250</td>
        <td>Entire cycles of training on the same data (more = possibly better learning, but not necessarily)</td>
      </tr>
    </table> 
    <br>
    The frame sizes 16, 64 correspond to the additional tiers of SampleRNN (the first tier is always n = 1, or consecutive samples). A 2-tier SampleRNN architecture has a wider temporal scale of learning at 16 frames, while a 3-tier SampleRNN architecture learns at 16 and 64 frames.

    <h1>Preliminary experiments: training on synthetic waveforms</h1>
    Since SampleRNN operates on samples, it means I can use non-musical synthetic waveforms to verify some basic behaviors. <b>Disclaimer</b> that this is just an attempt to peek at SampleRNN's black box decisions, and not a rigorous experimental method. Using MATLAB, I generated some different sequences and permutations of floating point vectors:
    <pre>
    fs = 16000; % doesn't really matter with such fake numbers
    x1 = repelem(0.3, 4800000);
    x2 = [repelem(0.3, 5*16000) repelem(0.75, 5*16000) repelem(0.3, 5*16000) repelem(0.75, 5*16000)];
    x3 = [0.3 0.75];
    x3_big = repmat(x3, 1, 24000000);
    x4 = [0.3 0.66];
    x4_big = repmat(x4, 1, 24000000);
    audiowrite('0_3.wav', x1, fs);
    audiowrite('0_3_0_75_5050.wav', x2, fs);
    audiowrite('0_3_0_75_interleaved_samplewise.wav', x3_big, fs);
    audiowrite('0_3_0_66_interleaved_samplewise.wav', x4_big, fs);
    </pre>
    The above commands create a variety of "fake" wav files containing some simple sequences of floats, which I then used as training inputs for SampleRNN.
    <h2>Training to fit a sequence of all 0.3</h2>
    The first thing I verified is whether we can simply train SampleRNN only on samples <code>[0.3, 0.3, ...]</code> and have it be 100% overfitted (and generate exactly only <code>[0.3, 0.3, ...]</code> as a result).
    The training command (which outputs an example generated clip that I used as <code>y[n]</code>) is as follows. I mimicked the real preprocessing and training steps:
    <pre>
    # chop up the fake wav clip of [0.3, 0.3, ...] into 8-second overlapping clips
    $ python ./chunk_audio.py --input_file ./0_3.wav --output_dir ./testdata-waveform-0.3/ --chunk_length 8000 --overlap 1000

    # train on the folder of [0.3, 0.3, ...]
    $ python train.py --id testdata-waveform-0.3 --data_dir ./testdata-waveform-0.3 --num_epochs 5 --batch_size 64 --sample_rate 16000

    # the displayed y[n] is emitted during training in "generated/testdata-waveform-0.3/testdata-waveform-0.3_epoch_1.wav"
    </pre>
    Within 2 epochs (~15 minutes of training), the loss was 0.0% and accuracy was 100.0%, indicating that the model learned quickly how to generate the value 0.3:
    <img src="prelim_0.3_overfit.png" width=800px/>
    Interestingly, the values of the generated waveform aren't exactly 0.3 but 0.306 - I don't know where that comes from but it could be from the round-trip through SampleRNN's quantization layers.

    <h2>Training on a mixture of 0.3 and 0.75</h2>
    My next test was to see what happened if I trained on a mixture of 0.3 and 0.75. Recall the creation command:
    <pre>
    x2 = [repelem(0.3, 5*16000) repelem(0.75, 5*16000) repelem(0.3, 5*16000) repelem(0.75, 5*16000)];
    </pre>
    At a sampling rate of 16000 Hz, this is 5 seconds of 0.3 followed by 5 seconds of 0.75 (repeated 2x overall). In addition to the clip emitted during training, I also generated an additional 4 clips. The results were as follows (<b>n.b.</b> this is a dense stem plot that makes it look like colored rectangles):
    <img src="prelim_0.3_0.75.png" width=800px/>
    Every generated clip had the value 0.306, and not 0.75. The model only learned how to generate 0.3s from observing a 50/50 split of monolithic sequences of <code>[0.3, ...]</code> and <code>[0.75, ...]</code>, which is an interesting outcome. I generated 10s of clips that all only contained 0.3.

    <h2>Training on interleaved 0.3,0.75 consecutive sample pairs</h2>
    The lowest and most important temporal scale of learning of SampleRNN is on the level of individual consecutive samples. An easy experiment to verify this is to create a sequence of <code>[0.3, 0.75, 0.3, 0.75, ...]</code> and validate that the output is similar:
    <img src="prelim_0.3_0.75_interleave.png" width=800px/>
    The output is the same, which is expected.

    <h2>Training on mixed interleaved 0.3,0.75 and 0.3,0.66 consecutive sample pairs</h2>
    In a similar experiment to mixing 0.3 and 0.75 (where the outcome was that only 0.3 was generated as an output), let's train on a 50/50 mix of interleaved pairs of 0.3,0.75 and 0.3,0.66:
    <img src="prelim_0.3_0.66_0.75_interleave.png" width=800px/>
    As we saw with the 50/50 mix of 0.3 and 0.75, the output seems to adhere to only one of the training inputs, and not both.

    <h2>Training with 16-sample temporal patterns</h2>
    Next up is to test the "tiers = temporal scales of learning" hypothesis. To recap, in the 3-tier SampleRNN, there should be learning at the level of consecutive samples, followed by learning on broader frames that are of size 16 and 64 samples. Let's create another fake waveform with a ramp that repeats at every 16 samples:
    <pre>
    x = linspace(0.1, 0.9, 16);
    x_big = repmat(x, 1, 2400000);
    audiowrite('frame_16_pattern.wav', x_big, 16000);
    </pre>
    Note that while this is very different from a real pattern in music, it should still count as a pattern in the waveform that SampleRNN can learn:
    <img src="prelim_ramp_16.png" width=800px/>
    This is one of my favorite results of the preliminary experiments, showing that SampleRNN could faithfully recreate the ramping pattern, but with some oddities (negative-valued samples).
    The accuracy and loss capped at the following values after 1 epoch:
    <pre>
    Epoch: 5/5, Step: 500/625, Loss: 1.386, Accuracy: 75.000, (0.445 sec/step)
    Epoch: 5/5, Step: 501/625, Loss: 1.386, Accuracy: 75.000, (0.429 sec/step)
    Epoch: 5/5, Step: 502/625, Loss: 1.386, Accuracy: 75.000, (0.426 sec/step)
    Epoch: 5/5, Step: 503/625, Loss: 1.386, Accuracy: 75.000, (0.427 sec/step)
    Epoch: 5/5, Step: 504/625, Loss: 1.386, Accuracy: 75.000, (0.425 sec/step)
    Epoch: 5/5, Step: 505/625, Loss: 1.386, Accuracy: 75.000, (0.426 sec/step)
    Epoch: 5/5, Step: 506/625, Loss: 1.386, Accuracy: 75.000, (0.428 sec/step)
    </pre>

    <h2>Training with 32-sample temporal patterns</h2>
    Let's repeat the above but with 32 samples. Since the frame sizes of learning are 16 and 64, what could the outcome of 32 be?
    <pre>
    x = linspace(0.1, 0.9, 32);
    x_big = repmat(x, 1, 1200000);
    audiowrite('frame_32_pattern.wav', x_big, 16000);
    </pre>
    An interesting note is that during the training I immediately noticed that even from the first epoch of training on 32-sample temporal patterns, the loss was lower and accuracy higher:
    <pre>
    Loading corpus entry ./testdata-waveform-32frame/frame_32_pattern_chunk_97.wav
    Loading corpus entry ./testdata-waveform-32frame/frame_32_pattern_chunk_50.wav
    Epoch: 1/5, Step: 250/625, Loss: 1.026, Accuracy: 82.249, (0.466 sec/step)
    Epoch: 1/5, Step: 251/625, Loss: 1.026, Accuracy: 82.257, (0.438 sec/step)
    Epoch: 1/5, Step: 252/625, Loss: 1.025, Accuracy: 82.266, (0.438 sec/step)
    Epoch: 1/5, Step: 253/625, Loss: 1.024, Accuracy: 82.274, (0.432 sec/step)
    Epoch: 1/5, Step: 254/625, Loss: 1.024, Accuracy: 82.282, (0.440 sec/step)
    Epoch: 1/5, Step: 255/625, Loss: 1.023, Accuracy: 82.290, (0.433 sec/step)
    Epoch: 1/5, Step: 256/625, Loss: 1.023, Accuracy: 82.299, (0.458 sec/step)
    Epoch: 1/5, Step: 257/625, Loss: 1.022, Accuracy: 82.307, (0.455 sec/step)
    Epoch: 1/5, Step: 258/625, Loss: 1.021, Accuracy: 82.315, (0.460 sec/step)
    </pre>
    It capped within 2 epochs at a higher value than the previous:
    <pre>
    Generated sample output to ./generated/testdata-waveform-32frame/testdata-waveform-32frame_epoch_1.wav
    Done
    Epoch: 2/5, Step: 1/625, Loss: 0.868, Accuracy: 84.375, (0.438 sec/step)
    Epoch: 2/5, Step: 2/625, Loss: 0.867, Accuracy: 84.375, (0.472 sec/step)
    Epoch: 2/5, Step: 3/625, Loss: 0.867, Accuracy: 84.375, (0.471 sec/step)
    </pre>
    This indicates that 32-sample temporal patterns are easier for the model to converge to than 16-samples. Here's the result generated waveform:
    <img src="prelim_ramp_32.png" width=800px/>

    <h2>Training with two "tiers" of temporal patterns</h2>
    Let's try a more complicated pattern - ramps that are 50 samples long, but with a larger pattern of "ramping ramps":
    <pre>
    x_1 = linspace(0.1, 0.2, 50);
    x_2 = linspace(0.1, 0.4, 50);
    x_3 = linspace(0.1, 0.6, 50);
    x_4 = linspace(0.1, 0.8, 50);

    x_big = repmat([x_1 x_2 x_3 x_4], 1, 100000);
    audiowrite('frame_double_pattern.wav', x_big, 16000);
    </pre>
    Loss and accuracy cap within 2 epochs:
    <pre>
    Loading corpus entry ./testdata-waveform-double-pattern/frame_double_pattern_chunk_310.wav
    Loading corpus entry ./testdata-waveform-double-pattern/frame_double_pattern_chunk_226.wav
    Loading corpus entry ./testdata-waveform-double-pattern/frame_double_pattern_chunk_175.wav
    Epoch: 2/5, Step: 1/625, Loss: 0.103, Accuracy: 98.438, (5.912 sec/step)
    Epoch: 2/5, Step: 2/625, Loss: 0.092, Accuracy: 98.486, (0.425 sec/step)
    Epoch: 2/5, Step: 3/625, Loss: 0.089, Accuracy: 98.503, (0.422 sec/step)
    Epoch: 2/5, Step: 4/625, Loss: 0.087, Accuracy: 98.511, (0.427 sec/step)
    </pre>
    Results:
    <img src="prelim_ramp_of_ramps.png" width=800px/>
    It looks to me as if SampleRNN overfitted (desired, in this case) very well to the ramp-of-ramps pattern that spanned multiple temporal timescales.

    <h2>Preliminary tests - thoughts</h2>
    We can see that 3-tier SampleRNN generalizes to several different waveform patterns, from simple 16-sample ramps to a 200-sample meta-pattern (ramp of ramps).
    <br>
    <br>
    One unexplained result is why, when presented with a 50/50 mix of different outcomes, SampleRNN adheres to only one of the training datasets and doesn't generate two different results. In the 50/50 tests, I would have expected at least 1 generated clip to only consist of <code>[0.75, ...]</code>.

    <h1>Experiment 0: training on a single album</h1>
    The next thing is to try it on real music - Animals as Leaders' self-titled album:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/SZ2WrN93vno" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    I downloaded the audio using youtube-dl, converted it to 16kHz mono with ffmpeg (recommended for SampleRNN to perform better), split it up into chunks (ignoring silence) using the prism-samplernn script <code>chunk_audio.py</code>, and ran the training with default parameters:
    <pre>
    $ python train.py --id aamgen --data_dir ./chunks/ --num_epochs 100 --batch_size 64 --checkpoint_every 5 --output_file_dur 3 --sample_rate 16000
    </pre>
    This emitted some generated clips during the training. Let's listen to 2 of the more musically interesting clips (a lot of them are just silence), generated at epoch 20 and 85 of the 100-epoch training. An epoch is one cycle of the entire training dataset - this means that the neural network observed the same album 100 times iteratively to learn how to model it:
    <br>
    Epoch 20:<br>
    <audio controls>
	    <source src="aamgen_epoch_20.wav" type="audio/wav"</source>
    </audio>
    <br>
    Epoch 85:<br>
    <audio controls>
	    <source src="aamgen_epoch_85.wav" type="audio/wav"</source>
    </audio>
    <br>
    <br>
    After the training was done (it took ~3 days on my machine), I generated 2 10-second clips of what I thought would be "Animas-as-Leaders-esque" music. The generate command is:
    <pre>
    $ python generate.py --output_path ./aamgen-out/aamgen.wav --checkpoint_path ./logdir/aamgen/14.09.2020_19.02.06/model.ckpt-90 --config_file ./default.config.json --num_seqs 2 --dur 10 --sample_rate 16000
    </pre>
    This says to use the model checkpoint 90. Even though we specified 100 epochs in the training, the model has an intelligent stop when it sees that additional training is not improving the model. In this case, it seems like 90 epochs exhausted the model's learning ability. Here's one of them (both sound equally bad):
    <br>
    <audio controls>
	    <source src="aamgen_0.wav" type="audio/wav"</source>
    </audio>

    <h2>Experiment 0 lessons</h2>
    I applied tweaks that Karl Hiner<sup><a href="#ref">[15]</a></sup> did in his experiments - for my next experiment, I would try some or all of the following:
    <ul>
	    <li>Increase the number of RNN layers from 4 to 5 to try to create more cohesive music</li>
	    <li>Use more training data than just one album</li>
	    <li>Increase epochs from 100 to 250 (longer training may lead to better results)</li>
    </ul>
    Also since the 3-tier architecture of the PRiSM-SampleRNN implementation may be producing worse music than 2-tier, a subsequent experiment I will attempt is to modify the PRiSM-SampleRNN implementation from 3 to 2 tiers.

    <h1>Experiment 1: longer training on multiple albums</h1>
    For my next experiment, I downloaded instrumental versions of the albums of Periphery (instrumental - I didn't want vocals mixing into the results, as I want to focus on musical instruments acoustics) and Mestis (an instrumental band). The data fetch and preprocessing scripts are available in my prism-samplernn fork:
    <pre>
    #!/usr/bin/env bash
    
    echo "Fetching training data - youtube-dl wav files for Mestis and Periphery albums"
    
    # youtube playlists for Mestis - Eikasia, Polysemy, Basal Ganglia
    mestis_album_1="PLNOrZEIoYAMgLJeZeCUEhABLPz7yqkyfI"
    mestis_album_2="PLfoVvOUi1CqV0O-yMdOvTff_vp8hOQnWi"
    mestis_album_3="PLRK89uMjq03BMsxBKFGBcDAh2G7ACwJMK"
    
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${mestis_album_1}
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${mestis_album_2}
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${mestis_album_3}
    
    # youtube playlists for instrumental Periphery albums - Periphery III, I, II, IV, Omega, Juggernaut
    periphery_album_1="PLSTnbYVfZR03JGmoJri6Sgvl4f0VAi9st"
    periphery_album_2="PL7DVODcLLjFplM5Rw-bNUyrwAECIPRK26"
    periphery_album_3="PLuEYu7jyZXdde7ePWV1RUvrpDKB8Gr6ex"
    periphery_album_45="PLEFyfJZV-vtKeBedXTv82yxS7gRZkzfWr"
    periphery_album_6="PL6FJ2Ri6gSpOWcbdq--P5J0IRcgH-4RVm"
    
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_1}
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_2}
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_3}
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_45}
    youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_6}
    
    mkdir -p periphery-raw
    mkdir -p mestis-raw
    
    find . -maxdepth 1 -mindepth 1 -type f -iname '*PERIPHERY*.wav' -exec mv {} periphery-raw/ \;
    find . -maxdepth 1 -mindepth 1 -type f -iname '*MESTIS*.wav' -exec mv {} mestis-raw/ \;
    find . -maxdepth 1 -mindepth 1 -type f -iname '*Javier*.wav' -exec mv {} mestis-raw/ \;
    find . -maxdepth 1 -mindepth 1 -type f -iname '*Suspiro*.wav' -exec mv {} mestis-raw/ \;
    find . -maxdepth 1 -mindepth 1 -type f -name '*.wav' -exec rm {} \;
    
    mkdir -p mestis-processed
    mkdir -p periphery-processed
    
    echo "Processing each wav file to 16kHz mono"
    
    for f in mestis-raw/*.wav; do
            ffmpeg -i "${f}" -ac 1 -ar 16000 "mestis-processed/$(basename "$f")";
    done
    
    for f in periphery-raw/*.wav; do
            ffmpeg -i "${f}" -ac 1 -ar 16000 "periphery-processed/$(basename "$f")";
    done
    
    mkdir -p periphery-chunks
    mkdir -p mestis-chunks
    mkdir -p mixed-chunks
    
    for f in mestis-processed/*.wav; do
            python ../chunk_audio.py --input_file "${f}" --output_dir mestis-chunks --chunk_length 8000 --overlap 1000
            python ../chunk_audio.py --input_file "${f}" --output_dir mixed-chunks --chunk_length 8000 --overlap 1000
    done
    
    for f in periphery-processed/*.wav; do
            python ../chunk_audio.py --input_file "${f}" --output_dir periphery-chunks --chunk_length 8000 --overlap 1000
            python ../chunk_audio.py --input_file "${f}" --output_dir mixed-chunks --chunk_length 8000 --overlap 1000
    done
    </pre>
    What the script does is:
    <ul>
	    <li>Fetch files for every Mestis song (from YouTube playlists + youtube-dl)</li>
	    <li>Fetch files for every instrumental Periphery song (from YouTube playlists + youtube-dl)</li>
	    <li>Pre-process them into 16kHz mono with ffmpeg (for optimal training)</li>
	    <li>Apply the chunk_audio.py script to split into non-silent 8-second chunks with 1 second overlap</li>
	    <li>Create 3 sets of training data - periphery-chunks, mestis-chunks, mixed-chunks</li>
    </ul>
    My intention was to train the model on each of the sets of training chunks, to create generated music that:
    <ul>
	    <li>Sounds like Periphery only</li>
	    <li>Sounds like Mestis only</li>
	    <li>Sounds like "Mestiphery", an organic mashup</li>
    </ul>
    <h2>Experiment 1 part 1 - Periphery only</h2>
    The training command for periphery-only is:
    <pre>
$ python train.py --id periphery_only --data_dir ./experiment-1/periphery-chunks/ --num_epochs 100 --batch_size 64 --sample_rate 16000
    </pre>
    
    <h3>Periphery only results</h3>
    Here's a 30-second clip output from the training on Periphery only:
    <br>
    <audio controls>
	    <source src="periphery_only_0.wav" type="audio/wav"</source>
    </audio>
    <br>
    <br>
    Another trait is that most generated audio consists of silence. I was very lucky to get almost 30 seconds of musical content in a single clip. Subjectively, this sounds nothing like Periphery:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/1_VZji3YFo4?start=90" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    Some more clips show the high-pitched output (which is melodic, but again, seemingly bizarre when trained on downtuned palm-muted rhythm guitar riffs).
    <br>
    Epoch 67:
    <br>
    <audio controls>
	    <source src="periphery_only_epoch_67.wav" type="audio/wav"</source>
    </audio>
    <br>
    Epoch 70:
    <br>
    <audio controls>
	    <source src="periphery_only_epoch_70.wav" type="audio/wav"</source>
    </audio>
    <br>
    <h4>Overfitting to a single song, Make Total Destroy</h4>
    Since SampleRNN was trained on 6 different albums, let's narrow down why it wasn't able to create Periphery's characteristic sound by overfitting specifically on the song shown above.
    <br>
    <br>
    After creating a test dataset with only the song Make Total Destroy and training on it for 100 epochs, the model reaches the following loss and accuracy:
    <pre>
    Epoch: 91/100, Step: 118/125, Loss: 0.867, Accuracy: 74.538, (0.246 sec/step)
    Epoch: 91/100, Step: 119/125, Loss: 0.864, Accuracy: 74.616, (0.255 sec/step)
    Epoch: 91/100, Step: 120/125, Loss: 0.861, Accuracy: 74.705, (0.247 sec/step)
    Epoch: 91/100, Step: 121/125, Loss: 0.857, Accuracy: 74.797, (0.246 sec/step)
    Epoch: 91/100, Step: 122/125, Loss: 0.854, Accuracy: 74.893, (0.246 sec/step)
    Epoch: 91/100, Step: 123/125, Loss: 0.850, Accuracy: 74.985, (0.251 sec/step)
    Epoch: 91/100, Step: 124/125, Loss: 0.847, Accuracy: 75.076, (0.246 sec/step)
    </pre>
    Generated audio from epoch 91:
    <br>
    Clip 1:
    <br>
    <audio controls>
	    <source src="maketotaldestroy_1.wav" type="audio/wav"</source>
    </audio>
    <br>
    Clip 2:
    <br>
    <audio controls>
	    <source src="maketotaldestroy_2.wav" type="audio/wav"</source>
    </audio>
    <br>
    These don't sound anything like music.

    <h3>Generated audio visualization</h3>
    The above clips show collections of realistic note onsets. One of my original statements about WaveNet and SampleRNN was that they could produce music with convincing dynamics and timbre, to make us believe real humans played it. Observing various aspects of the waveform in the time and frequency domain should be useful.
    <br>
    <br>
    Let's view each clip (epochs 67, 70) in the time domain and frequency domain (with a spectrogram):
    <img src="periphery_epoch_67_timedomain.png" width=800px/>
    <img src="periphery_epoch_70_timedomain.png" width=800px/>
    <img src="periphery_epoch_67_spectrogram.png" width=800px/>
    <img src="periphery_epoch_70_spectrogram.png" width=800px/>
    <br>
    Although this is subjective, one can see the dynamic nature of the produced audio in the plots above. It really does look like there are real musical variations in the complex waveform (aside from the totally blank silences which are odd in real music).

    <h3>Mu-law vs linear quantization</h3>
    Karl Hiner's blog post touches on WaveNet's mu-law quantization, and claims it sounds better than SampleRNN's linear quantization. In fact, I found that every SampleRNN implementation I found had options for linear and mu-law quantization (perhaps it was added later). In fact the original ICLR 2017 paper even has an "a-law quantization" (similar to mu-law). Let's hear what each sounds like:
    <br>
    <br>
    Periphery epoch 83, mu-law quantization:
    <br>
    <audio controls>
	    <source src="periphery_epoch_83_mulaw.wav" type="audio/wav"</source>
    </audio>
    <br>
    Periphery epoch 83, linear quantization <b>WARNING! LOUD!</b>:
    <br>
    <audio controls>
	    <source src="periphery_epoch_83_linear.wav" type="audio/wav"</source>
    </audio>
    <br>
    In my subjective listening test, the linear quantization output is very loud, almost to the point of distortion and clipping. The mu-law quantization is outputting music with more subtle volume, possibly since its more suited for the logarithmic human experience of loudness.

    <h2>Abandoning the original hypotheses</h2>
    It took 1 week to complete the training on Periphery only. My limitation is computational power. There are issues today about "AI democratization", and my experiments demonstrate the difficulty of black-box computational models where the large companies or labs publishing papers have massive advantages in hardware and in-house neural network/machine learning expertise (FIND REFERENCE FAANGMAN NEURAL NETWORKS).
    <br>
    <br>
    The results were very different from my expectations:
    <ul>
      <li>The resulting generated audio is mostly silence and junk</li>
      <li>There are some interesting potentially musical sounds, but it's all high-pitched whistling and doesn't contain any characteristics of the band Periphery (palm-muted distorted guitar chords, etc.)</li>
      <li>I have to generate 100s or 1000s of clips and curate the results to create a final result - the chances of getting 1 cohesive "song" (let's say, 2 minutes of contiguous music) are pretty slim</li>
    </ul>
    With the demonstrated poor quality of results (e.g. generated audio that sounds nothing like the band), my original hypotheses were debunked. I mixed the Mestis data into the Periphery data, and continued training the model (that was initially trained only on Periphery for 100 epochs) for 250 epochs. At this point my goal was to "embrace the weird" and see what sort of strange music I can create. The sum total of all my training and experiments (including failed starts) for 3-tier SampleRNN was <b>3 weeks</b>.
    <h2>Training results</h2>
    The results of training SampleRNN are stored in the <code>logdir</code> directory, in timestamped directories storing checkpoints for intermediate epochs in training. Here's a <code>tree</code> view of of the training. The size of the training directory is 51GB, after having been trained on all of the albums listed above (709MB of music):
    <pre>
    $ tree logdir/periphery_only/ -L 2
    logdir/periphery_only/
    ├── 15.09.2020_17.36.46
    │   ├── checkpoint
    │   ├── model.ckpt-79.data-00000-of-00001
    │   ├── model.ckpt-79.index
    │   ├── model.ckpt-80.data-00000-of-00001
    │   ├── model.ckpt-80.index
    │   ├── model.ckpt-81.data-00000-of-00001
    │   ├── model.ckpt-81.index
    │   ├── model.ckpt-82.data-00000-of-00001
    │   ├── model.ckpt-82.index
    │   ├── model.ckpt-83.data-00000-of-00001
    │   ├── model.ckpt-83.index
    │   └── train
    ├── 17.09.2020_21.55.43
    │   ├── checkpoint
    │   ├── model.ckpt-89.data-00000-of-00001
    │   ├── model.ckpt-89.index
    │   ├── model.ckpt-90.data-00000-of-00001
    │   ├── model.ckpt-90.index
    │   ├── model.ckpt-91.data-00000-of-00001
    │   ├── model.ckpt-91.index
    │   ├── model.ckpt-92.data-00000-of-00001
    │   ├── model.ckpt-92.index
    │   ├── model.ckpt-93.data-00000-of-00001
    │   ├── model.ckpt-93.index
    │   └── train
    ...truncated...
    └── 22.09.2020_22.25.13
        ├── checkpoint
        ├── model.ckpt-244.data-00000-of-00001
        ├── model.ckpt-244.index
        ├── model.ckpt-245.data-00000-of-00001
        ├── model.ckpt-245.index
        ├── model.ckpt-246.data-00000-of-00001
        ├── model.ckpt-246.index
        ├── model.ckpt-247.data-00000-of-00001
        ├── model.ckpt-247.index
        ├── model.ckpt-248.data-00000-of-00001
        ├── model.ckpt-248.index
        └── train
    </pre>
    <h1>Experiment 2: does 2-tier SampleRNN make better music?</h1>
    The next step is to use the SampleRNN implementation most successful for producing music - that of the dadabots. Recall that my earlier hestitation for using the dadabots code was how old the dependencies were. As such, it was slightly trickier to get up and running than the modern Tensorflow 2 PRiSM fork.
    <h2>Dadabots SampleRNN setup</h2>
    The dadabots SampleRNN installation instructions<sup><a href="#ref">https://github.com/Cortexelus/dadabots_sampleRNN/wiki/Installing-Dadabots-SampleRNN-on-Ubuntu</a></sup> are tailored to a Google Cloud Platform Ubuntu 16.04 setup with an NVIDIA V100 GPU. I need to figure out installation steps that work on my Fedora 32 desktop computer with a different GPU.
    <br>
    <br>
    The full install steps for a functional dadabots SampleRNN (with notes) were as follows:
    <pre>
    # we need python 2.7 for dadabots sampleRNN
    # create a conda environment

    $ conda create -n dadabots_SampleRNN python=2.7 anaconda
    $ conda activate dadabots_SampleRNN

    # we need Theano==1.0.0 - newer breaks

    (dadabots_SampleRNN) $ conda install -c mila-udem -c mila-udem/label/pre theano==1.0.0 pygpu

    # we need Lasagne (based on Theano) 0.2.dev1
    # this command installs 0.2.dev1 correctly - suggested by https://github.com/imatge-upc/salgan/issues/29

    (dadabots_SampleRNN) $ pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip

    # we need to create a ~/.theanorc file with some cudnn details
    # cudnn.h is installed on my system through Fedora repos, as mentioned previously
    # 
    # nvcc requires an older GCC, 8.4, which I already had set up

    $ cat ~/.theanorc
    [dnn]
    include_path = /usr/include/cuda

    [global]
    mode = FAST_RUN
    device = cuda0
    floatX = float32

    [nvcc]
    compiler_bindir = /home/sevagh/GCC-8.4.0/bin

    # at this point, we have a working dadabots 2-tier SampleRNN model
    # install some extra pip packages

    (dadabots_SampleRNN) $ pip install graphviz pydot pydot-ng
    </pre>

    <h2>Training data preparation</h2>

    <h1>Curating neural generated results</h1>
    An important thing to note is that the Dadabots focus heavily on curating<sup><a href="#ref">[16]</a></sup> the resultant audio clips. From a blackbox neural network that produces any kind of audio, from silence, to a cacophony of sounds, and everything in between, one needs to curate the results to combine them into a cohesive piece of music.
    <br>
    <br>
    The goal of my automated curation script is to:
    <ul>
	    <li>Load every clip generated by the trained SampleRNN model</li>
	    <li>Trim out the silence</li>
	    <li>Potentially apply some MIR techniques* to group similar clips together, to create the sense of a "cohesive" musical piece</li>
	    <li>Concatenate the result clips of sound with crossfading</li>
    </ul>
    *Ideally, we would use popular MIR Python libraries (librosa, madmom, Essentia) to assist us in the curation step, e.g. by grouping similar clips by their musical content. However, as this is more in the domain of MUMT 621 (music information retrieval), and not an easy task, I won't spend too much time on it.
    <br>
    <br>
    I want to minimize the amount of visual/manual audio curation that I do, and rely only on the simple curation listed above.
    <h2>Generating weird music with the 3-tier trained model</h2>
    First, I use the trained model described earlier to generate audio clips at various epochs. I chose the epochs randomly - the ones before checkpoint 100 are trained only on Periphery, while the ones after checkpoint 100 are trained on a mix of Periphery and Mestis (aka "Mestiphery").
    <br>
    <br>
    According to the dadabots' README<sup><a href="#ref">[18]</a></sup>, they don't necessarily accept that the latest training epoch is the best one. That's why I picked a variety of epochs:
    <blockquote>
      However, we found the latest checkpoint does not always create the best music. Instead we listen to the test audio generated at each checkpoint, choose our favorite checkpoint, and delete the newer checkpoints, before generating a huge batch with this script.
    </blockquote>
    I generated sequences of different durations. The generate commands are as follows:
    <pre>
    # epoch 83, 92 (periphery only)
    
    $ python generate.py --output_path ./to-curate-periphery-epoch-83/gen.wav --checkpoint_path ./logdir/periphery_only/15.09.2020_17.36.46/model.ckpt-83 --config_file ./default.config.json --num_seqs 10 --dur 30 --sample_rate 16000
    $ python generate.py --output_path ./to-curate-periphery-epoch-83-2/gen.wav --checkpoint_path ./logdir/periphery_only/15.09.2020_17.36.46/model.ckpt-83 --config_file ./default.config.json --num_seqs 100 --dur 8 --sample_rate 16000
    $ python generate.py --output_path ./to-curate-periphery-epoch-92/gen.wav --checkpoint_path ./logdir/periphery_only/17.09.2020_21.55.43/model.ckpt-92 --config_file ./default.config.json --num_seqs 10 --dur 30 --sample_rate 16000
    
    # epoch 139, 165, 188 (mestiphery - mixed periphery + mestis)
    
    $ python generate.py --output_path ./to-curate-mestiphery-epoch-139/gen.wav --checkpoint_path ./logdir/periphery_only/18.09.2020_04.32.49/model.ckpt-139 --config_file ./default.config.json --num_seqs 10 --dur 30 --sample_rate 16000
    $ python generate.py --output_path ./to-curate-mestiphery-epoch-165/gen.wav --checkpoint_path ./logdir/periphery_only/20.09.2020_10.04.13/model.ckpt-165 --config_file ./default.config.json --num_seqs 100 --dur 8 --sample_rate 16000
    $ python generate.py --output_path ./mestiphery-to-curate-epoch-188/gen.wav --checkpoint_path ./logdir/periphery_only/20.09.2020_17.12.32/model.ckpt-188 --config_file ./default.config.json --num_seqs 30 --dur 10 --sample_rate 16000
    </pre>
    The results of these generation commands are directories named "to-curate-*" full of wav files:
    <pre>
    $ ls to-curate-*
    to-curate-mestiphery-epoch-139:
    gen_0.wav  gen_1.wav  gen_2.wav  gen_3.wav  gen_4.wav  gen_5.wav  gen_6.wav  gen_7.wav  gen_8.wav  gen_9.wav
    
    to-curate-mestiphery-epoch-165:
    gen_0.wav   gen_28.wav  gen_46.wav  gen_64.wav  gen_82.wav
    gen_10.wav  gen_29.wav  gen_47.wav  gen_65.wav  gen_83.wav
    ...truncated...
    
    to-curate-mestiphery-epoch-188:
    gen_0.wav   gen_12.wav  gen_15.wav  gen_18.wav  gen_20.wav  gen_23.wav  gen_26.wav  gen_29.wav  gen_4.wav  gen_7.wav
    gen_10.wav  gen_13.wav  gen_16.wav  gen_19.wav  gen_21.wav  gen_24.wav  gen_27.wav  gen_2.wav   gen_5.wav  gen_8.wav
    gen_11.wav  gen_14.wav  gen_17.wav  gen_1.wav   gen_22.wav  gen_25.wav  gen_28.wav  gen_3.wav   gen_6.wav  gen_9.wav
    
    to-curate-mestiphery-epoch-92:
    gen_0.wav  gen_1.wav  gen_2.wav  gen_3.wav  gen_4.wav  gen_5.wav  gen_6.wav  gen_7.wav  gen_8.wav  gen_9.wav
    
    to-curate-periphery-epoch-83:
    gen_0.wav  gen_2.wav  gen_4.wav  gen_6.wav  gen_8.wav
    gen_1.wav  gen_3.wav  gen_5.wav  gen_7.wav  gen_9.wav
    
    to-curate-periphery-epoch-83-2:
    gen_0.wav   gen_28.wav  gen_46.wav  gen_64.wav  gen_82.wav
    gen_10.wav  gen_29.wav  gen_47.wav  gen_65.wav  gen_83.wav
    gen_11.wav  gen_2.wav   gen_48.wav  gen_66.wav  gen_84.wav
    </pre>
    After listening to several of the files manually, I noticed that the lower epochs produce better, more cohesive, and longer bits of music. This to me indicates the failure of experiment 2, of mixing Periphery + Mestis and training for 250 epochs. This could be due to the nature of the training data - the neural network is trying to generate samples that minimize error for two distinct musical artists and styles, so it can resort to outputting nothing (to minimize numerical error between the two sets of training data). Also, like the Dadabots say, perhaps training for too long isn't beneficial.
    <h2>Chromaprint-based curation script</h2>
    The first tool I use in the curation script is librosa's trim silence<sup><a href="#ref">[20]</a></sup> to remove leading and trailing silence from the generated clips. This helps as the majority of clips have some musical content but lots of silence in between.
    <br>
    <br>
    The next thing I need is to be able to compare the pieces of musical content/non-silence with each other, to concatenate clips by similarity. This should have the effect of creating a maximally cohesive piece of music. I settled for the chromaprint<sup><a href="#ref">[21] chromaprint</a></sup>. Using chromaprints, we can get an array of 32-bit integers representing the "acoustic fingerprint" of a waveform.
    <br>
    <br>
    Combining these two elements is the first job of the curation script:
    <pre>
    for wav_file in os.listdir(p):
        full_path = os.path.join(p, wav_file)
        print('trimming silence and computing chromaprint for {0}'.format(full_path))
    
        x, _ = soundfile.read(full_path, dtype='float32')
    
        # trim trailing and leading silence
        x_trimmed, _ = librosa.effects.trim(x, top_db=50, frame_length=256, hop_length=64)
    
        fingerprint = es.Chromaprinter(sampleRate=args.sample_rate)(x_trimmed)
        int_fingerprint = ai.chromaprint.decode_fingerprint(bytes(fingerprint, encoding='utf-8'))[0]
    
        all_audio[full_path] = {
                'raw_audio': x_trimmed,
                'chromaprint': int_fingerprint,
        }
    </pre>
    This code extracts sequences of non-silent music from the generated clips, and stores their chromaprints. We also need a way to compare the arrays of 32-bit integer chromaprints, to be able to rank the generated music clips by their similarity with other clips. For this I borrowed some code that computes a correlation between lists of integer chromaprints<sup><a href="#ref">[22]</a></sup>. I rank clips by maximum correlation in their chromaprints:
    <pre>
    # naive O(n^2) comparison
    for filename1, data1 in all_audio.items():
        for filename2, data2 in all_audio.items():
            print('comparing chromaprint correlation for {0}, {1}'.format(filename1, filename2))
            if filename1 == filename2:
                # don't compare a file to itself
                continue
            try:
                chromaprint_correlation = correlation(data1['chromaprint'], data2['chromaprint'])
            except:
                continue
            correlation_scores[chromaprint_correlation] = (filename1, filename2)
    </pre>
    The last part of the script sorts clips by maximum correlation in their chromaprints, and accumulates them in a "total_curated.wav" file:
    <pre>
    # sort by the most highly correlated pairs of audio
    sorted_correlation_scores = dict(sorted(correlation_scores.items(), reverse=True))
    total_audio = None
    for v in sorted_correlation_scores.values():
        print('concatenating audio by similarity')
        # if we've already taken a clip before, ignore
        if v[0] not in all_audio.keys() or v[1] not in all_audio.keys():
            continue
    
        # keep a running accumulation of similar clips
        if total_audio is None:
            total_audio = all_audio[v[0]]['raw_audio']
        else:
            total_audio = numpy.concatenate((total_audio, all_audio[v[0]]['raw_audio']))
        total_audio = numpy.concatenate((total_audio, all_audio[v[1]]['raw_audio']))
    
        # delete the data we don't need anymore
        del all_audio[v[0]]
        del all_audio[v[1]]
    
    print('writing output file')
    soundfile.write("total_curated.wav", total_audio, args.sample_rate)
    </pre>
    This gives a 24-minute song (when run on all of the generated clips mentioned previously). Although cacophonous and random, it actually sounds like (very weird) music. Also, it emphatically <b>does not</b> sound like Periphery or Mestis, the two bands which supplied the training data.
    <h2>Coral Reefer</h2>
    When life gives you lemons, you make lemonade. Given a 24-minute vague, experimental piece of music, I decided to take it all the way and apply several effects to maximize the impression that it's a piece created by a quirky human musician. The title of the song is "Coral Reefer", and the effects I want to apply are a mix of guitar effects from the stoner metal genre, and some aquatic/underwater effects to fit with the artist theme of "1000 sharks".
    <br>
    <br>
    Effects:
    <ul>
      <li>phaser</li>
      <li>flanger</li>
    </ul>
    <h1>Album art generation</h1>
    SampleRNN, which I've shown so far, is a model for unconditional music generation. NVIDIA's StyleGAN2<sup><a href="#ref">stylegan2</a></sup> is a model for unconditional image generation. I'll use StyleGAN2 to create the fake album art for 1000sharks. Given that both are unconditional waveform generators (audio = 1D waveform with an implicit time axis, image = 2D matrix, non-temporal), the training and generation procedures are broadly similar to what has been described.
    <h2>Image pre-preprocessing script</h2>
    StyleGAN2 expects the training data to be square images with the same power-of-two dimension. I wrote a Python script that automatically extracts the square middle <code>dim</code> pixels of an image and saves them as png files using Pillow<sup><a href="#ref">pil</a></sup>:
    <pre>
    from PIL import Image, ImageOps

    seq = 0
    # dim is a user-supplied argument

    for p in args.inpaths:
        for image in os.listdir(p):
            img = Image.open(os.path.join(p, image))

            thumbnail = ImageOps.fit(
                img,
                (args.dim, args.dim),
                Image.ANTIALIAS
            )
            thumbnail.save(os.path.join(args.outpath, '{0}.png'.format(seq)))
            seq += 1
    </pre>
    I've committed this script (crop_images.py) to my fork of StyleGAN2<sup><a href="#ref">my-stylegan2-fork</a></sup>. One thing to note is that I had to run <code>conda install libwebp</code> in my Conda environment before installing Pillow to support the webp image format.
    <br>
    <br>
    Early on this page I mentioned vague "difficulties" when relying on older machine learning libraries. I encountered many of these with StyleGAN2:
    <ul>
      <li>I needed to install Python 3.7 which is the last version of Python that still supports Tensorflow 1.15 (an older version required for StyleGAN2)</li>
      <li>I had to symlink several CUDA libraries to the 10.0 versions expected by Tensorflow 1.15:
        <pre>
        ln -snf /usr/lib64/libcudart.so.10.2.89 /usr/lib64/libcudart.so.10.0
        ln -snf /usr/lib64/libcublas.so.10.2.89 /usr/lib64/libcublas.so.10.0
        ln -snf /usr/lib64/libcufft.so.10.1.2.89 /usr/lib64/libcufft.so.10.0
        ln -snf /usr/lib64/libcublas.so.10.2.2.89 /usr/lib64/libcublas.so.10.0
        ln -snf /usr/lib64/libcusparse.so.10.3.1.89 /usr/lib64/libcusparse.so.10.0
        ln -snf /usr/lib64/libcurand.so.10.1.2.89 /usr/lib64/libcurand.so.10.0
        ln -snf /usr/lib64/libcusolver.so.10.3.0.89 /usr/lib64/libcusolver.so.10.0
        </pre>
      </li>
      <li>After encountering several resource exhaustion crashes, I had to allow GPU memory growth, similar to the modification I had to make to SampleRNN: <code>export TF_FORCE_GPU_ALLOW_GROWTH="true"</code> </li>
    </ul>
    <h2>Preprocessing, training, and generation commands</h2>
    The training data consists of shark images (saved from a Google image search), and heavy metal album covers (saved from the following article<sup><a href="#ref">article https://www.loudersound.com/features/the-50-best-death-metal-albums-ever</a></sup>.
    <br>
    <br>
    After downloading these to a directory, I ran the following commands:
    <pre>
    # create 256x256 middle cropped images from sharks and album covers
    $ python crop_images.py --dim=256 ./output-images/ ./shark-images/ ./metal-album-covers/

    # preprocess cleaned cropped images using stylegan2's own tool
    $ python dataset_tool.py create_from_images datasets/1000sharks/ ./output-images/

    # train for config-e and kimg=1000
    $ python run_training.py --data-dir=./datasets/ --dataset=1000sharks --config=config-e --total-kimg=1000

    # generate 1000 images, randomly seeded, for curation
    $ python run_generator.py generate-images --seeds=0-999 --truncation-psi=1.0   --network=results/00008-stylegan2-1000sharks-1gpu-config-e/network-final.pkl
    </pre>
    I chose the training parameters <code>config-e</code> (the second-best configuration - config-f is the best configuration in StyleGAN2 but is a larger neural network that's much slower to train - these are explained <a href="link-to-think">TODO HERE</a>), with kimg=1000 (this is similar to the epochs of SampleRNN, in that more is probably better and increases the training time).
    <br>
    <br>
    After 31 hours of training, the model was done:
    <pre>
    tick 118   kimg 951.7    lod 0.00  minibatch 32   time 1d 06h 10m   sec/tick 845.5   sec/kimg 104.85  maintenance 0.0    gpumem 5.0
    tick 119   kimg 959.7    lod 0.00  minibatch 32   time 1d 06h 24m   sec/tick 845.2   sec/kimg 104.81  maintenance 0.0    gpumem 5.0
    tick 120   kimg 967.8    lod 0.00  minibatch 32   time 1d 06h 38m   sec/tick 844.9   sec/kimg 104.78  maintenance 0.0    gpumem 5.0
    network-snapshot-000967        time 11m 39s      fid50k 192.4708
    tick 121   kimg 975.9    lod 0.00  minibatch 32   time 1d 07h 04m   sec/tick 844.7   sec/kimg 104.76  maintenance 712.7  gpumem 5.0
    tick 122   kimg 983.9    lod 0.00  minibatch 32   time 1d 07h 18m   sec/tick 845.8   sec/kimg 104.89  maintenance 0.0    gpumem 5.0
    tick 123   kimg 992.0    lod 0.00  minibatch 32   time 1d 07h 32m   sec/tick 845.7   sec/kimg 104.87  maintenance 0.0    gpumem 5.0
    tick 124   kimg 1000.1   lod 0.00  minibatch 32   time 1d 07h 47m   sec/tick 845.6   sec/kimg 104.87  maintenance 0.0    gpumem 5.0
    network-snapshot-001000        time 11m 41s      fid50k 195.7975
    dnnlib: Finished training.training_loop.training_loop() in 1d 07h 59m.
    </pre>
    The logs to stdout are similar to SampleRNN (kimg = epoch, fid50k = accuracy measure where lower is better<a href="otherthing">https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/</a>).
    <h2>Curated album art</h2>
    There are 9 results I liked the most and included in the project:
    <br>
    <img src="curated_album_art_1.png"/>
    <img src="curated_album_art_2.png"/>
    <img src="curated_album_art_3.png"/>
    <img src="curated_album_art_4.png"/>
    <img src="curated_album_art_5.png"/>
    <img src="curated_album_art_6.png"/>
    <img src="curated_album_art_7.png"/>
    <img src="curated_album_art_8.png"/>
    <img src="curated_album_art_9.png"/>
    <h1>Results & conclusion</h1>

    <h1 id="ref">References</h2>
    <ol>
      <li>mlacoustics (citations.bib)</li>
      <li>wavenetpaper (citations.bib)</li>
      <li>samplernnpaper (citations.bib)</li>
      <li>magentaddsp (citations.bib)</li>
      <li>dadabots (citations.bib)</li>
      <li>new ref: machine learning reproducability crisis https://petewarden.com/2018/03/19/the-machine-learning-reproducibility-crisis</li>
      <li>prism (citations.bib)</li>
      <li>prism-blog (citations.bib)</li>
      <li>prism gh commits</li>
      <li>my fork</li>
      <li>tf weird fix https://github.com/search?q=%22tf.config.experimental.set_memory_growth%28physical_devices%5B0%5D%2C+enable%3DTrue%29%22&type=issues</li>
      <li> conda </li>
      <li> dadabots curation </li>
      <li> librosa trim https://librosa.org/doc/main/generated/librosa.effects.trim.html </li>
      <li> chromaprint https://acoustid.org/chromaprint </li>
      <li> https://medium.com/@shivama205/audio-signals-comparison-23e431ed2207 </li>
    </ol>
  </body>
</html>

