<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="GitLab Pages">
    <title>1000 sharks - an AI music experience</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body id="index">
    <div class="navbar">
      <a href="./">MUMT 618 report</a>
      <a href="./demo.html">1000sharks demo</a>
    </div>

    <h1>1000 sharks</h1>
    <h3>An AI music experience, prepared by Sevag for MUMT 618, Fall 2020</h3>

    <p>
    Machine learning and deep learning techniques in recent years have an increasing presence in the field of musical acoustics<sup><a href="#ref">[1]</a></sup>. As a preliminary introduction to generating music using neural networks, you can view my <a href="paper_presentation_final.pdf" type="application/pdf" title="Neural audio synthesis">paper presentation</a> introducing WaveNet<sup><a href="#ref">[2]</a></sup>, SampleRNN<sup><a href="#ref">[3]</a></sup>, and other novel approaches to neural audio.
    <br>
    <br>
    In the presentation, I outlined that there exist both unconstrained, unstructured models for generating audio in the waveform domain (resulting in babbling or confusing outputs), and structured physical computational models that have been used in traditional audio synthesis (that don't sound very natural).
    <br>
    <br>
    In this project, I'll use SampleRNN for unstructured neural audio synthesis. From this diagram, I'll be targeting the top left image:
    <figure>
    <img src="6_5_ml_acoustics.jpg" width=400px/>
    <figcaption>By augmenting ML methods (top left) with physical models (bottom right) to obtain hybrid models (upper right), a synergy of the strengths of physical intuition and data-driven insights can be obtained <sup><a href="#ref">[1]</a></sup></figcaption>
    </figure>
    </p>

    <h1>Motivation</h1>

    <p>
    The dadabots<sup><a href="#ref">[5]</a></sup> have been creating music with neural networks, trained on the music of specific artists:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/MwtVkPKx3RA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    My initial idea was to see if I could imitate the dadabots by training SampleRNN in a variety of experiments:
    <ol>
      <li>Train on a single album and generate music, just to familiarize myself with SampleRNN</li>
      <li>Two different artists, trained separately, to generate music that sounds like Artist A and Artist B</li>
      <li>Combined training on both artists, to generate music that sounds like Artist A + Artist B</li>
      <li>Training on separate instruments to layer the results over each other</li>
      <li>Training on separate song segments (e.g. choruses, verses, guitar solos) and concatenating the generated outputs to make a Frankensong</li>
    </ol>
    To supplement the illusion of a "real" music artist, I also want to generate album art using additional AI techniques; as these are not directly related to musical acoustics, they'll occupy a small section at the bottom of this report.
    <br>
    <br>
    To explain the overall aesthetic/theme of death metal + sharks:
    <iframe width="300" height="256" src="https://www.youtube.com/embed/EzCbsw3WgTk?start=85" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

    <h1>Deeper dives</h1>
    Despite the "black box" nature of WaveNet and SampleRNN, we can do a below-the-surface examination of the models (including training and generation steps) to get a better sense of:
    <ol>
	    <li>Input data and preprocessing - how are the training waveforms represented?</li>
	    <li>The model/neural network itself - what are WaveNet and SampleRNN computing?</li>
	    <li>Loss function - how do WaveNet/SampleRNN know that one set of parameters is better than the other? What defines the "correct" output of a waveform?</li>
	    <li>Generation - after training a model with low loss, how do WaveNet and SampleRNN use the trained model to generate brand new waveforms of audio?</li>
    </ol>
    <a href="/wavenet_overview.html"><h2>WaveNet overview</h2></a>

    <h1>Experiment setup</h1>
    A difficult step in any machine learning experiment is actually reproducing the claims of papers<sup><a href="#ref">[6]</a></sup>. Most of the models have outdated dependencies. You most likely can't run something that was published in 2017, as the popular machine learning frameworks on which most papers depend (Torch, Keras, Tensorflow) are evolving at a fast pace.
    <br>
    <br>
    As an example, I tried to unsuccessfully run the original SampleRNN code, and many other forks, until finding a good one:
    <ol>
      <li>https://github.com/soroushmehr/sampleRNN_ICLR2017</li>
      <li>https://github.com/worosom/docker-zvk-SampleRNN</li>
      <li>https://github.com/ZVK/sampleRNN_ICLR2017</li>
      <li><b>The dadabots' own fork</b>: https://github.com/Cortexelus/dadabots_sampleRNN/wiki/Installing-Dadabots-SampleRNN-on-Ubuntu</li>
      <li>https://github.com/deepsound-project/samplernn-pytorch</li>
    </ol>

    I finally found the RNCM (Royal Northern College of Music) PRiSM (Practice and Research in Science and Music) lab released a modern implementation of 3-tier SampleRNN<sup><a href="#ref">[7]</a></sup>:
    <blockquote>
    PRiSM is shortly going to publish its own implementation, using TensorFlow 2, and we’ll be explaining the features of the PRiSM SampleRNN in our next instalment – when we will also make the code available on PRiSM’s GitHub pages, along with a number of pretrained and optimised models.
    </blockquote>

    <h2>Hardware and OS setup</h2>

    I ran all of the training and experiments on my personal desktop computer, consisting of:
    <ul>
      <li>AMD Ryzen 7 3700X 8c/16t processor</li>
      <li>32GB RAM</li>
      <li>1TB NVME storage</li>
      <li>NVIDIA RTX 2070 SUPER GPU</li>
    </ul>
    The OS is Fedora 32, and NVIDIA drivers and CUDA toolkit were installed using negativo17's Fedora-nvidia repositories<sup><a href="#ref">[11]</a></sup>.

    <a href="./samplernn_experiments.html"><h2>SampleRNN experiments</h2></a>
    
    <a href="./curation.html"><h1>Curating neural generated results</h1></a>

    <a href="./album_art.html"><h1>Album art generation</h1></a>

    <h1>Results & conclusion</h1>

    <h1 id="ref">References</h2>
    <ol>
      <li>mlacoustics (citations.bib)</li>
      <li>wavenetpaper (citations.bib)</li>
      <li>samplernnpaper (citations.bib)</li>
      <li>magentaddsp (citations.bib)</li>
      <li>dadabots (citations.bib)</li>
      <li>new ref: machine learning reproducability crisis https://petewarden.com/2018/03/19/the-machine-learning-reproducibility-crisis</li>
      <li>prism (citations.bib)</li>
      <li>prism-blog (citations.bib)</li>
      <li>prism gh commits</li>
      <li>my fork</li>
      <li>tf weird fix https://github.com/search?q=%22tf.config.experimental.set_memory_growth%28physical_devices%5B0%5D%2C+enable%3DTrue%29%22&type=issues</li>
      <li> conda </li>
      <li> dadabots curation </li>
      <li> librosa trim https://librosa.org/doc/main/generated/librosa.effects.trim.html </li>
      <li> chromaprint https://acoustid.org/chromaprint </li>
      <li> https://medium.com/@shivama205/audio-signals-comparison-23e431ed2207 </li>
    </ol>
  </body>
</html>

