<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="GitLab Pages">
    <title>1000 sharks - an AI music experience</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="navbar">
      <a href="./">MUMT 618 report</a>
      <a href="./demo">1000sharks demo</a>
    </div>

    <h1>1000 sharks</h1>
    <h3>An AI music experience, prepared by Sevag for MUMT 618</h3>

    <p>
    Machine learning and deep learning techniques in recent years have an increasing presence in the field of musical acoustics<sup><a href="#ref">[1]</a></sup>. As a preliminary introduction to generating music using neural networks, you can view my <a href="paper_presentation_final.pdf" type="application/pdf" title="Neural audio synthesis">paper presentation</a> introducing WaveNet<sup><a href="#ref">[2]</a></sup>, SampleRNN<sup><a href="#ref">[3]</a></sup>, and other novel approaches to neural audio.
    <br>
    <br>
    In the presentation, I outlined that there exist both unconstrained, unstructured models for generating audio in the waveform domain (resulting in babbling or confusing outputs), and structured physical computational models that have been used in traditional audio synthesis (that don't sound very natural).
    <br>
    <br>
    A mixed approach of using machine and deep learning to estimate physical model parameters trained on real data is a good middle ground. In this project, I'll be using SampleRNN for unstructured neural audio synthesis. I'll also explore the Magenta Differentiable DSP library<sup><a href="#ref">[4]</a></sup> for a hybrid machine learning + physical model approach. From this diagram, I'll be targeting the top left and top right images:
    <figure>
    <img src="6_5_ml_acoustics.jpg" width=400px/>
    <figcaption>By augmenting ML methods (top left) with physical models (bottom right) to obtain hybrid models (upper right), a synergy of the strengths of physical intuition and data-driven insights can be obtained <sup><a href="#ref">[1]</a></sup></figcaption>
    </figure>
    </p>

    <h1>Motivation</h1>

    <p>
    The dadabots<sup><a href="#ref">[5]</a></sup> have been creating music with neural networks, trained on the music of specific artists:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/MwtVkPKx3RA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    My initial idea was to see if I could imitate the dadabots by training SampleRNN in a variety of experiments:
    <ol>
      <li>Train on a single album and generate music, just to familiarize myself with SampleRNN</li>
      <li>Two different artists, trained separately, to generate music that sounds like Artist A and Artist B</li>
      <li>Combined training on both artists, to generate music that sounds like Artist A + Artist B</li>
      <li>Training on separate instruments to layer the results over each other</li>
      <li>Training on separate song segments (e.g. choruses, verses, guitar solos) and concatenating the generated outputs to make a Frankensong</li>
    </ol>
    To supplement the illusion of a "real" music artist, I also want to generate album art using additional AI techniques; as these are not directly related to musical acoustics, they'll occupy a small section at the bottom of this report.
    </p>

    <h1>Experiment setup</h1>
    A difficult step in any machine learning experiment is actually reproducing the claims of papers<sup><a href="#ref">[6]</a></sup>. Most of the models have outdated dependencies. You most likely can't run something that was published in 2017, as the popular machine learning frameworks on which most papers depend (Torch, Keras, Tensorflow) are evolving at a breakneck pace.
    <br>
    <br>
    As an example, I tried to unsuccessfully run the original SampleRNN code, and many other forks, until finding a good one:
    <ol>
      <li>https://github.com/soroushmehr/sampleRNN_ICLR2017</li>
      <li>https://github.com/worosom/docker-zvk-SampleRNN</li>
      <li>https://github.com/ZVK/sampleRNN_ICLR2017</li>
      <li><b>The dadabots' own fork</b>: https://github.com/Cortexelus/dadabots_sampleRNN/wiki/Installing-Dadabots-SampleRNN-on-Ubuntu</li>
      <li>https://github.com/deepsound-project/samplernn-pytorch</li>
    </ol>

    I finally lucked out and found the RNCM (Royal Northern College of Music) PRiSM (Practice and Research in Science and Music) lab released a modern implementation of SampleRNN with Tensorflow 2<sup><a href="#ref">[7]</a></sup>.
    <br>
    <br>
    To demonstrate the futility in trying to do this myself, one can read the accompanying PRiSM blog post<sup><a href="#ref">[8]</a></sup>:
    <blockquote>
    PRiSM is shortly going to publish its own implementation, using TensorFlow 2, and we’ll be explaining the features of the PRiSM SampleRNN in our next instalment – when we will also make the code available on PRiSM’s GitHub pages, along with a number of pretrained and optimised models.
    </blockquote>
    If we check the GitHub commits<sup><a href="#ref">[9]</a></sup>, we can see that it took the researcher, Dr. Christopher Melen, around 9 months to get the repository into the shape it was in when I was able to get it running. For me, as a complete beginner to machine learning, to have done the same, is highly unlikely. This gives me slight unease at how unfriendly machine learning models are to beginners.

    <h2>Hardware and OS setup</h2>

    I ran all of the training and experiments on my personal desktop computer, consisting of:
    <ul>
      <li>AMD Ryzen 7 3700X 8c/16t processor</li>
      <li>32GB RAM</li>
      <li>1TB NVME storage</li>
      <li>NVIDIA RTX 2070 SUPER GPU</li>
    </ul>
    The OS is Fedora 32, and NVIDIA drivers and CUDA toolkit were installed using negativo17's Fedora-nvidia repositories<sup><a href="#ref">[11]</a></sup>.

    <h2>Python setup + minor code tweaks</h2>

    I forked the prism-samplernn codebase to my own GitHub profile to make a minor adjustment in the scripts<sup><a href="#ref">[12]</a></sup>.
    <pre>
physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)
    </pre>
    Without this parameter, the training would crash on my GPU (RTX 2070 SUPER) with a mysterious error message, "Fail to find the dnn implementation". It's an esoteric fix that one can find scattered across GitHub<sup><a href="#ref">[13]</a></sup>.
    <br>
    <br>
    The Python setup is straightforward using conda<sup><a href="#ref">[14]</a></sup> and following the project's README.md:
    <pre>
$ conda create -n prism-samplernn python=3.8 anaconda
$ conda activate prism-samplernn
(prism-samplernn) $ pip install -r requirements.txt
    </pre>

    <h1>Experiment 0: training on a single album</h1>
    The first thing I tried was to train SampleRNN on Animals as Leaders' self-titled album:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/SZ2WrN93vno" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    I downloaded the audio using youtube-dl, converted it to 16kHz mono with ffmpeg (recommended for SampleRNN to perform better), split it up into chunks (ignoring silence) using the prism-samplernn script <code>chunk_audio.py</code>, and ran the training with default parameters:
    <pre>
$ python train.py --id aamgen --data_dir ./chunks/ --num_epochs 100 --batch_size 64 --checkpoint_every 5 --output_file_dur 3 --sample_rate 16000
    </pre>
    This emitted some generated clips during the training. Let's listen to 2 of the more musically interesting clips (a lot of them are just silence), generated at epoch 20 and 85 of the 100-epoch training. An epoch is one cycle of the entire training dataset - this means that the neural network observed the same album 100 times iteratively to learn how to model it:
    <br>
    Epoch 20:<br>
    <audio controls>
	    <source src="aamgen_epoch_20.wav" type="audio/wav"</source>
    </audio>
    <br>
    Epoch 85:<br>
    <audio controls>
	    <source src="aamgen_epoch_85.wav" type="audio/wav"</source>
    </audio>
    <br>
    <br>
    After the training was done (it took ~3 days on my machine), I generated 2 10-second clips of what I thought would be "Animas-as-Leaders-esque" music. The generate command is:
    <pre>
$ python generate.py --output_path ./aamgen-out/aamgen.wav --checkpoint_path ./logdir/aamgen/14.09.2020_19.02.06/model.ckpt-90 --config_file ./default.config.json --num_seqs 2 --dur 10 --sample_rate 16000
    </pre>
    This says to use the model checkpoint 90. Even though we specified 100 epochs in the training, the model has an intelligent stop when it sees that additional training is not improving the model. In this case, it seems like 90 epochs exhausted the model's learning ability. Here's one of them (both sound equally bad):
    <br>
    <audio controls>
	    <source src="aamgen_0.wav" type="audio/wav"</source>
    </audio>
    <h2>Experiment 0 lessons</h2>
    Recall that in the SampleRNN paper, the multiple layers of the RNN determined the learning of audio patterns at different temporal scales:
    <figure>
    <img src="8_samplernn.png" width=700px/>
    <figcaption>Higher RNN layers map to wider temporal scales <sup><a href="#ref">[3]</a></sup></figcaption>
    </figure>
    <br>
    The lowest temporal scale (consecutive samples) represents very low-level audio features (e.g. timbre), while higher scales can (hypothetically) go as far as representing repeating choruses or verses minutes apart.
    <br>
    With this in mind, I applied tweaks that Karl Hiner<sup><a href="#ref">[15]</a></sup> did in his experiments - for my next experiment, I would:
    <ul>
	    <li>increase epochs from 100 to 250 (longer training should lead to better results)</li>
	    <li>increase the number of RNN layers from 4 to 5 to try to create more cohesive music</li>
	    <li>Use more training data than just one album</li>
    </ul>
    <h1>Experiment 1: longer training on multiple albums</h1>
    For my next experiment, I downloaded instrumental versions of the albums of Periphery (instrumental - I didn't want vocals mixing into the results, as I want to focus on musical instruments acoustics) and Mestis (an instrumental band). The data fetch and preprocessing scripts are available in my prism-samplernn fork:
    <pre>
#!/usr/bin/env bash

echo "Fetching training data - youtube-dl wav files for Mestis and Periphery albums"

# youtube playlists for Mestis - Eikasia, Polysemy, Basal Ganglia
mestis_album_1="PLNOrZEIoYAMgLJeZeCUEhABLPz7yqkyfI"
mestis_album_2="PLfoVvOUi1CqV0O-yMdOvTff_vp8hOQnWi"
mestis_album_3="PLRK89uMjq03BMsxBKFGBcDAh2G7ACwJMK"

youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${mestis_album_1}
youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${mestis_album_2}
youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${mestis_album_3}

# youtube playlists for instrumental Periphery albums - Periphery III, I, II, IV, Omega, Juggernaut
periphery_album_1="PLSTnbYVfZR03JGmoJri6Sgvl4f0VAi9st"
periphery_album_2="PL7DVODcLLjFplM5Rw-bNUyrwAECIPRK26"
periphery_album_3="PLuEYu7jyZXdde7ePWV1RUvrpDKB8Gr6ex"
periphery_album_45="PLEFyfJZV-vtKeBedXTv82yxS7gRZkzfWr"
periphery_album_6="PL6FJ2Ri6gSpOWcbdq--P5J0IRcgH-4RVm"

youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_1}
youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_2}
youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_3}
youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_45}
youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_6}

mkdir -p periphery-raw
mkdir -p mestis-raw

find . -maxdepth 1 -mindepth 1 -type f -iname '*PERIPHERY*.wav' -exec mv {} periphery-raw/ \;
find . -maxdepth 1 -mindepth 1 -type f -iname '*MESTIS*.wav' -exec mv {} mestis-raw/ \;
find . -maxdepth 1 -mindepth 1 -type f -iname '*Javier*.wav' -exec mv {} mestis-raw/ \;
find . -maxdepth 1 -mindepth 1 -type f -iname '*Suspiro*.wav' -exec mv {} mestis-raw/ \;
find . -maxdepth 1 -mindepth 1 -type f -name '*.wav' -exec rm {} \;

mkdir -p mestis-processed
mkdir -p periphery-processed

echo "Processing each wav file to 16kHz mono"

for f in mestis-raw/*.wav; do
        ffmpeg -i "${f}" -ac 1 -ar 16000 "mestis-processed/$(basename "$f")";
done

for f in periphery-raw/*.wav; do
        ffmpeg -i "${f}" -ac 1 -ar 16000 "periphery-processed/$(basename "$f")";
done

mkdir -p periphery-chunks
mkdir -p mestis-chunks
mkdir -p mixed-chunks

for f in mestis-processed/*.wav; do
        python ../chunk_audio.py --input_file "${f}" --output_dir mestis-chunks --chunk_length 8000 --overlap 1000
        python ../chunk_audio.py --input_file "${f}" --output_dir mixed-chunks --chunk_length 8000 --overlap 1000
done

for f in periphery-processed/*.wav; do
        python ../chunk_audio.py --input_file "${f}" --output_dir periphery-chunks --chunk_length 8000 --overlap 1000
        python ../chunk_audio.py --input_file "${f}" --output_dir mixed-chunks --chunk_length 8000 --overlap 1000
done
    </pre>
    What the script does is:
    <ul>
	    <li>Fetch files for every Mestis song (from YouTube playlists + youtube-dl)</li>
	    <li>Fetch files for every instrumental Periphery song (from YouTube playlists + youtube-dl)</li>
	    <li>Pre-process them into 16kHz mono with ffmpeg (for optimal training)</li>
	    <li>Apply the chunk_audio.py script to split into non-silent 8-second chunks with 1 second overlap</li>
	    <li>Create 3 sets of training data - periphery-chunks, mestis-chunks, mixed-chunks</li>
    </ul>
    My intention was to train the model on each of the sets of training chunks, to create generated music that:
    <ul>
	    <li>Sounds like Periphery only</li>
	    <li>Sounds like Mestis only</li>
	    <li>Sounds like "Mestiphery", an organic mashup</li>
    </ul>
    <h2>Experiment 1 part 1 - Periphery only</h2>
    The training command for periphery-only is:
    <pre>
$ python train.py --id periphery_only --data_dir ./experiment-1/periphery-chunks/ --num_epochs 250 --batch_size 64 --sample_rate 16000
    </pre>
    It took <b>1 week</b> to complete the training on Periphery only. I bold 1 week because, in an academic semester of 3 months, taking 8% of the semester to run one single experiment in a large project concerns me. My limitation is computational power. I could have taken advantage of expensive hardware (e.g. using the resources of my future research lab, DDMAL), but I felt that this would go against the spirit of an independent student project.
    <br>
    <br>
    Also, the conclusion that running all of my envisioned experiments is impossible in the time frame of a semester is actually a valuable outcome. It shows the difficulty of black-box computational models where the large companies or labs publishing papers are leaps and bounds ahead of casual readers due to the unprecedented compute power available to them(FIND REFERENCE FAANGMAN NEURAL NETWORKS).
    <h3>Periphery only results</h3>
    Here's a 30-second clip output from the training on Periphery only:
    <br>
    <audio controls>
	    <source src="periphery_only_0.wav" type="audio/wav"</source>
    </audio>
    <br>
    <br>
    Another disappointing trait is that most generated audio consists of silence. I was very lucky to get almost 30 seconds of musical content in a single clip. Subjectively, this sounds nothing like Periphery:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/1_VZji3YFo4?start=90" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    This nullifies my hypothesis, that it's possible to expect a neural network trained (by an amateur) for 1 week to sound like a band of real professional musicians. A trend I witnessed in all of the Periphery generated clips is the lack of palm-muted distorted guitar chords (a sound very characteristic to Periphery) in the final outputs. All of the harmonic content seems to be a high-pitched whistling, but there are convincing drum sounds:
    <br>
    <br>
    Epoch 70:
    <br>
    <audio controls>
	    <source src="periphery_only_epoch_70.wav" type="audio/wav"</source>
    </audio>
    <br>
    Epoch 67:
    <br>
    <audio controls>
	    <source src="periphery_only_epoch_67.wav" type="audio/wav"</source>
    </audio>
    <br>
    Epoch 230:
    <br>
    <audio controls>
	    <source src="periphery_only_epoch_230.wav" type="audio/wav"</source>
    </audio>
    <br>

    <h2>Generated audio deep dive</h2>
    The above clips show collections of realistic note onsets. One of my original statements about WaveNet and SampleRNN was that they could produce music with convincing dynamics and timbre, to make us believe real humans played it. Observing various aspects of the waveform in the time and frequency domain should be useful.
    <br>
    <br>
    Let's view each clip (67, 70, 230) in the time domain and frequency domain (with a spectrogram):
    <img src="periphery_epoch_67_timedomain.png" width=800px/>
    <img src="periphery_epoch_70_timedomain.png" width=800px/>
    <img src="periphery_epoch_230_timedomain.png" width=800px/>
    <img src="periphery_epoch_67_spectrogram.png" width=800px/>
    <img src="periphery_epoch_70_spectrogram.png" width=800px/>
    <img src="periphery_epoch_230_spectrogram.png" width=800px/>
    <br>
    Although this is subjective, one can see the dynamic nature of the produced audio in the plots above. It really does look like there are real musical variations in the complex waveform.

    <h2>Experimenting with mu-law quantization</h2>
    Something else I mentioned in the paper presentation was how WaveNet used mu-law quantization while SampleRNN didn't. Karl Hiner remarked in his experiments that the SampleRNN results sounded "bit-crushed" due to the poor linear quantization when compared with WaveNet's mu-law quantization results. The PRiSM SampleRNN fork actually supports both linear and mu-law quantization for generated audio.
    <br>
    <br>
    Let's generate some test clips with linear and mu-law quantization to see the difference for ourselves, by passing a "mu-law.config.json" and "linear.config.json" config files to the generate.py script respectively. We can also set the generated output sample rate to 48kHz (to avoid the low 16kHz rate affecting our judgement):
    <pre>
$ python generate.py --output_path ./peripherygen-linear-quant --checkpoint_path ./logdir/periphery_only/22.09.2020_22.25.13/model.ckpt-248 --config_file ./linear-quant.config.json --num_seqs 2 --dur 5 --sample_rate 16000
...output truncated...
Generated samples 79361 - 79616 of 80000 (time elapsed: 91.430 seconds)
Generated samples 79617 - 79872 of 80000 (time elapsed: 91.704 seconds)
Generated samples 79873 - 80000 of 80000 (time elapsed: 91.981 seconds)
Generated sample output to ./peripherygen-linear-quant_0.wav
Generated sample output to ./peripherygen-linear-quant_1.wav
Done
$ python generate.py --output_path ./peripherygen-mulaw-quant --checkpoint_path ./logdir/periphery_only/22.09.2020_22.25.13/model.ckpt-248 --config_file ./mu-law.config.json --num_seqs 2 --dur 5 --sample_rate 16000
...output truncated...
Generated samples 79617 - 79872 of 80000 (time elapsed: 91.949 seconds)
Generated samples 79873 - 80000 of 80000 (time elapsed: 92.223 seconds)
Generated sample output to ./peripherygen-mulaw-quant_0.wav
Generated sample output to ./peripherygen-mulaw-quant_1.wav
Done
    </pre>
    Note that generating samples is much faster than training. It takes 3 minutes to generate the 4 5-second 16kHz clips for comparing linear to mu-law quantization. Let's observe the outputs:
    <br>
    <br>
    Linear quant:
    <br>
    <audio controls>
	    <source src="peripherygen-linear-quant_0.wav" type="audio/wav"</source>
    </audio>
    <br>
    <audio controls>
	    <source src="peripherygen-linear-quant_1.wav" type="audio/wav"</source>
    </audio>
    <br>
    These are mostly completely junk.
    <br>
    <br>
    mu-law quant:
    <br>
    <audio controls>
	    <source src="peripherygen-mulaw-quant_0.wav" type="audio/wav"</source>
    </audio>
    <br>
    <audio controls>
	    <source src="peripherygen-mulaw-quant_1.wav" type="audio/wav"</source>
    </audio>
    <br>
    These generated clips are mostly completely junk, and indicate the problem with SampleRNN - it's really hard to tell whether the output will contain real musical content.

    <h2>Rolling Experiment 2+ into experiment 1</h2>
    As noted, experiment 1 took <b>1 week</b> to train, or 8% of the Fall semester. To top it off, the results were very divorced from my expectations:
    <ul>
      <li>The resulting generated audio is mostly silence and junk</li>
      <li>There are some interesting potentially musical sounds, but it's all high-pitched whistling and doesn't contain any characteristics of the band Periphery (palm-muted distorted guitar chords, etc.)</li>
      <li>I have to generate 100s or 1000s of clips and curate the results to create a final result - the chances of getting 1 cohesive "song" (let's say, 2 minutes of contiguous music) are pretty slim</li>
    </ul>
    With these in mind, I didn't see any value in performing training on separate bands, and I didn't think I'd have enough time to train on separate instruments or song segments. Instead, what I did was mix the Mestis data into the Periphery data, and continue training the model (that was initially trained only on Periphery for 250 epochs). This took another week, bringing the sum total of all generative experiments (including failed starts) to <b>3 weeks</b>. 3 weeks of letting my computer crunch data to compute a model that produces strange, confusing, and mostly silent outputs was a disappointing experience.

    <h1>Salvaging neural audio results with curation</h1>
    An important thing to note is that the Dadabots never claimed that their model spit out perfect music. They focus heavily on curating<sup><a href="#ref">[16]</a></sup> the resultant audio clips. From a blackbox neural network that produces any kind of audio, from silence, to a cacophony of sounds, and everything in between, one needs to curate the results to combine them into an aesthetically pleasing - or at least cohesive - piece of music.
    <br>
    <br>
    The goal of my automated curation script is to:
    <ul>
	    <li>Load every clip generated by the trained SampleRNN model</li>
	    <li>Group clips together based on their musical content (i.e. mostly musical clips should float to the top of the list, mostly junk and silence to the bottom)</li>
	    <li>Try to categorize the clips based on their musical similarity to each other</li>
	    <li>Concatenate the audio clips using ffmpeg crossfading</li>
    </ul>
    This is more in the domain of MUMT 621 (music information retrieval), so I won't spend too much time on it. Ideally, we can use popular MIR Python libraries (Librosa, madmom, Essentia) to assist us in the curation step - I want to minimize the amount of visual/manual audio curation that I do, and rely mostly only on code.

    <h1>Supplementing neural audio with structured synthesis</h1>
    <h2>Magenta and Differentiable DSP</h2>
    <h2>Training Karplus-Strong parameters with machine learning</h2>
    <h2>Creating some pieces of synthesized music with trained Karplus-Strong + algorithmic music generation</h2>
    <h1>Album art generation</h1>
    <h1>Putting it all together</h1>
    <h1>Final result</h1>
    <h1>Conclusion</h1>

    <h1 id="ref">References</h2>
    <ol>
      <li>mlacoustics (citations.bib)</li>
      <li>wavenetpaper (citations.bib)</li>
      <li>samplernnpaper (citations.bib)</li>
      <li>magentaddsp (citations.bib)</li>
      <li>dadabots (citations.bib)</li>
      <li>new ref: machine learning reproducability crisis https://petewarden.com/2018/03/19/the-machine-learning-reproducibility-crisis</li>
      <li>prism (citations.bib)</li>
      <li>prism-blog (citations.bib)</li>
      <li>prism gh commits</li>
      <li>my fork</li>
      <li>tf weird fix https://github.com/search?q=%22tf.config.experimental.set_memory_growth%28physical_devices%5B0%5D%2C+enable%3DTrue%29%22&type=issues</li>
      <li> conda </li>
      <li> dadabots curation </li>
    </ol>
  </body>
</html>

