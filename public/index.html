<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="GitLab Pages">
    <title>1000 sharks - an AI music experience</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="navbar">
      <a href="./">MUMT 618 report</a>
      <a href="./demo">1000sharks demo</a>
    </div>

    <h1>1000 sharks</h1>
    <h3>An AI music experience, prepared by Sevag for MUMT 618</h3>

    <p>
    Machine learning and deep learning techniques in recent years have an increasing presence in the field of musical acoustics<sup><a href="#ref">[1]</a></sup>. As a preliminary introduction to generating music using neural networks, you can view my <a href="paper_presentation_final.pdf" type="application/pdf" title="Neural audio synthesis">paper presentation</a> introducing WaveNet<sup><a href="#ref">[2]</a></sup>, SampleRNN<sup><a href="#ref">[3]</a></sup>, and other novel approaches to neural audio.
    <br>
    <br>
    In the presentation, I outlined that there exist both unconstrained, unstructured models for generating audio in the waveform domain (resulting in babbling or confusing outputs), and structured physical computational models that have been used in traditional audio synthesis (that don't sound very natural).
    <br>
    <br>
    A mixed approach of using machine and deep learning to estimate physical model parameters trained on real data is a good middle ground. In this project, I'll be using SampleRNN for unstructured neural audio synthesis. I'll also explore the Magenta Differentiable DSP library<sup><a href="#ref">[4]</a></sup> for a hybrid machine learning + physical model approach. From this diagram, I'll be targeting the top left and top right images:
    <figure>
    <img src="6_5_ml_acoustics.jpg" width=400px/>
    <figcaption>By augmenting ML methods (top left) with physical models (bottom right) to obtain hybrid models (upper right), a synergy of the strengths of physical intuition and data-driven insights can be obtained <sup><a href="#ref">[1]</a></sup></figcaption>
    </figure>
    </p>

    <h1>Motivation</h1>

    <p>
    The dadabots<sup><a href="#ref">[5]</a></sup> have been creating music with neural networks, trained on the music of specific artists:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/MwtVkPKx3RA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    My initial idea was to see if I could imitate the dadabots by training SampleRNN in a variety of experiments:
    <ol>
      <li>Train on a single album and generate music, just to familiarize myself with SampleRNN</li>
      <li>Two different artists, trained separately, to generate music that sounds like Artist A and Artist B</li>
      <li>Combined training on both artists, to generate music that sounds like Artist A + Artist B</li>
      <li>Training on separate instruments to layer the results over each other</li>
      <li>Training on separate song segments (e.g. choruses, verses, guitar solos) and concatenating the generated outputs to make a Frankensong</li>
    </ol>
    To supplement the illusion of a "real" music artist, I also want to generate album art using additional AI techniques; as these are not directly related to musical acoustics, they'll occupy a small section at the bottom of this report.
    </p>

    <h1>Experiment setup</h1>
    A difficult step in any machine learning experiment is actually reproducing the claims of papers<sup><a href="#ref">[6]</a></sup>. Most of the models have outdated dependencies. You most likely can't run something that was published in 2017, as the popular machine learning frameworks on which most papers depend (Torch, Keras, Tensorflow) are evolving at a breakneck pace.
    <br>
    <br>
    As an example, I tried to unsuccessfully run the original SampleRNN code, and many other forks, until finding a good one:
    <ol>
      <li>https://github.com/soroushmehr/sampleRNN_ICLR2017</li>
      <li>https://github.com/worosom/docker-zvk-SampleRNN</li>
      <li>https://github.com/ZVK/sampleRNN_ICLR2017</li>
      <li><b>The dadabots' own fork</b>: https://github.com/Cortexelus/dadabots_sampleRNN/wiki/Installing-Dadabots-SampleRNN-on-Ubuntu</li>
      <li>https://github.com/deepsound-project/samplernn-pytorch</li>
    </ol>

    I finally lucked out and found the RNCM (Royal Northern College of Music) PRiSM (Practice and Research in Science and Music) lab released a modern implementation of SampleRNN with Tensorflow 2<sup><a href="#ref">[7]</a></sup>.
    <br>
    <br>
    To demonstrate the futility in trying to do this myself, one can read the accompanying PRiSM blog post<sup><a href="#ref">[8]</a></sup>:
    <blockquote>
    PRiSM is shortly going to publish its own implementation, using TensorFlow 2, and we’ll be explaining the features of the PRiSM SampleRNN in our next instalment – when we will also make the code available on PRiSM’s GitHub pages, along with a number of pretrained and optimised models.
    </blockquote>
    If we check the GitHub commits<sup><a href="#ref">[9]</a></sup>, we can see that it took the researcher, Dr. Christopher Melen, around 9 months to get the repository into the shape it was in when I was able to get it running. For me, as a complete beginner to machine learning, to have done the same, is highly unlikely. This gives me slight unease at how unfriendly machine learning models are to beginners.
    

    <h2>Hardware and OS setup</h2>

    I ran all of the training and experiments on my personal desktop computer, consisting of:
    <ul>
      <li>AMD Ryzen 7 3700X 8c/16t processor</li>
      <li>32GB RAM</li>
      <li>1TB NVME storage</li>
      <li>NVIDIA RTX 2070 SUPER GPU</li>
    </ul>
    The OS is Fedora 32, and NVIDIA drivers and CUDA toolkit were installed using negativo17's Fedora-nvidia repositories<sup><a href="#ref">[11]</a></sup>.

    <h2>Python setup + minor code tweaks</h2>

    I forked the prism-samplernn codebase to my own GitHub profile to make a minor adjustment in the scripts<sup><a href="#ref">[12]</a></sup>.
    <pre>
physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)
    </pre>
    Without this parameter, the training would crash on my GPU (RTX 2070 SUPER) with a mysterious error message, "Fail to find the dnn implementation". It's an esoteric fix that one can find scattered across GitHub<sup><a href="#ref">[13]</a></sup>.
    <br>
    <br>
    The Python setup is straightforward using conda<sup><a href="#ref">[14]</a></sup> and following the project's README.md:
    <pre>
$ conda create -n prism-samplernn python=3.8 anaconda
$ conda activate prism-samplernn
(prism-samplernn) $ pip install -r requirements.txt
    </pre>

    <h1>Experiment 0: training on a single album</h1>
    The first thing I tried was to train SampleRNN on Animals as Leaders' self-titled album:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/SZ2WrN93vno" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    I downloaded the audio using youtube-dl, converted it to 16kHz mono with ffmpeg (recommended for SampleRNN to perform better), split it up into chunks (ignoring silence) using the prism-samplernn script <code>chunk_audio.py</code>, and ran the training with default parameters:
    <pre>
$ python train.py --id aamgen --data_dir ./chunks/ --num_epochs 100 --batch_size 64 --checkpoint_every 5 --output_file_dur 3 --sample_rate 16000
    </pre>
    This emitted some generated clips during the training. Let's listen to 2 of the more musically interesting clips (a lot of them are just silence), generated at epoch 20 and 85 of the 100-epoch training. An epoch is one cycle of the entire training dataset - this means that the neural network observed the same album 100 times iteratively to learn how to model it:
    <br>
    Epoch 20:<br>
    <audio controls>
	    <source src="aamgen_epoch_20.wav" type="audio/wav"</source>
    </audio>
    <br>
    Epoch 85:<br>
    <audio controls>
	    <source src="aamgen_epoch_85.wav" type="audio/wav"</source>
    </audio>
    <br>
    <br>
    After the training was done (it took ~3 days on my machine), I generated 2 10-second clips of what I thought would be "Animas-as-Leaders-esque" music. The generate command is:
    <pre>
$ python generate.py --output_path ./aamgen-out/aamgen.wav --checkpoint_path ./logdir/aamgen/14.09.2020_19.02.06/model.ckpt-90 --config_file ./default.config.json --num_seqs 2 --dur 10 --sample_rate 16000
    </pre>
    This says to use the model checkpoint 90. Even though we specified 100 epochs in the training, the model has an intelligent stop when it sees that additional training is not improving the model. In this case, it seems like 90 epochs exhausted the model's learning ability. Here's one of them (both sound equally bad):
    <br>
    <audio controls>
	    <source src="aamgen_0.wav" type="audio/wav"</source>
    </audio>

    <h2>Experiment 0 lessons</h2>
    Recall that in the SampleRNN paper, the multiple layers of the RNN determined the learning of audio patterns at different temporal scales:
    <figure>
    <img src="8_samplernn.png" width=700px/>
    <figcaption>Higher RNN layers map to wider temporal scales <sup><a href="#ref">[3]</a></sup></figcaption>
    </figure>
    <br>
    The lowest temporal scale (consecutive samples) represents very low-level audio features (e.g. timbre), while higher scales can (hypothetically) go as far as representing repeating choruses or verses minutes apart.
    <br>
    With this in mind, I applied tweaks that Karl Hiner<sup><a href="#ref">[15]</a></sup> did in his experiments - for my next experiment, I would try some or all of the following:
    <ul>
	    <li>Increase the number of RNN layers from 4 to 5 to try to create more cohesive music</li>
	    <li>Use more training data than just one album</li>
	    <li>Increase epochs from 100 to 250 (longer training may lead to better results)</li>
    </ul>

    <h2>SampleRNN hyperparameters table</h2>
    *Hyperparameter in machine learning = something that a human must set in the model, the model can't learn it on its own. As opposed to parameters, which are the internal magic numbers that are computed
    <br>
    <br>
    I'll summarize the available SampleRNN hyperparameters and other customizeable steps compared across the original 2017 ICLR implementation, the Dadabots fork, the PRiSM fork which I use throughout the rest of this report, and finally my own modifications to the PRiSM parameters after experiment 0:
    <br>
    <br>
    <table>
      <tr>
        <th></th>
        <th>Original</th>
        <th>Dadabots</th>
        <th>PRiSM</th>
        <th>Mine</th>
        <th>Descr</th>
      </tr>
      <tr>
        <th>RNN layers</th>
        <td>4</td>
        <td>5</td>
        <td>4</td>
        <td>5</td>
        <td>Temporal timescale of learning (more layers = learn patterns on wider timescale)</td>
      </tr>
      <tr>
        <th>Tiers</th>
        <td>2 or 3</td>
        <td>2 or 3 (2 recommended)</td>
        <td>3</td>
        <td>3</td>
        <td>Tiers of RNN (more = wider temporal timescale)</td>
      </tr>
      <tr>
        <th>Frame sizes (corresponds to tiers)</th>
        <td>16</td>
        <td>16</td>
        <td>16,64</td>
        <td>16,64</td>
        <td>Samples apart between low and high timescales</td>
      </tr>
      <tr>
        <th>Sample rate</th>
        <td>16000 (fixed)</td>
        <td>16000</td>
        <td>16000</td>
        <td>16000</td>
        <td>Sample rate of training/generating waveform (lower = faster learning, better able to learn long-timescale patterns)</td>
      </tr>
      <tr>
        <th>Training input</th>
        <td>No details</td>
        <td>Chop albums into 8s + 1s overlap</td>
        <td>Chop albums into 8s + 1s overlap</td>
        <td>Chop albums into 8s + 1s overlap</td>
        <td>Suggestions on how to prepare training data</td>
      </tr>
      <tr>
        <th>Epochs</th>
        <td>Not customizeable</td>
        <td>Not customizeable</td>
        <td>100</td>
        <td>100, 250</td>
        <td>Entire cycles of training on the same data (more = possibly better learning, but not necessarily)</td>
      </tr>
    </table> 
    <h1>Experiment 1: longer training on multiple albums</h1>
    For my next experiment, I downloaded instrumental versions of the albums of Periphery (instrumental - I didn't want vocals mixing into the results, as I want to focus on musical instruments acoustics) and Mestis (an instrumental band). The data fetch and preprocessing scripts are available in my prism-samplernn fork:
    <pre>
#!/usr/bin/env bash

echo "Fetching training data - youtube-dl wav files for Mestis and Periphery albums"

# youtube playlists for Mestis - Eikasia, Polysemy, Basal Ganglia
mestis_album_1="PLNOrZEIoYAMgLJeZeCUEhABLPz7yqkyfI"
mestis_album_2="PLfoVvOUi1CqV0O-yMdOvTff_vp8hOQnWi"
mestis_album_3="PLRK89uMjq03BMsxBKFGBcDAh2G7ACwJMK"

youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${mestis_album_1}
youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${mestis_album_2}
youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${mestis_album_3}

# youtube playlists for instrumental Periphery albums - Periphery III, I, II, IV, Omega, Juggernaut
periphery_album_1="PLSTnbYVfZR03JGmoJri6Sgvl4f0VAi9st"
periphery_album_2="PL7DVODcLLjFplM5Rw-bNUyrwAECIPRK26"
periphery_album_3="PLuEYu7jyZXdde7ePWV1RUvrpDKB8Gr6ex"
periphery_album_45="PLEFyfJZV-vtKeBedXTv82yxS7gRZkzfWr"
periphery_album_6="PL6FJ2Ri6gSpOWcbdq--P5J0IRcgH-4RVm"

youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_1}
youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_2}
youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_3}
youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_45}
youtube-dl -ci -f "bestaudio" -x --audio-format wav -i ${periphery_album_6}

mkdir -p periphery-raw
mkdir -p mestis-raw

find . -maxdepth 1 -mindepth 1 -type f -iname '*PERIPHERY*.wav' -exec mv {} periphery-raw/ \;
find . -maxdepth 1 -mindepth 1 -type f -iname '*MESTIS*.wav' -exec mv {} mestis-raw/ \;
find . -maxdepth 1 -mindepth 1 -type f -iname '*Javier*.wav' -exec mv {} mestis-raw/ \;
find . -maxdepth 1 -mindepth 1 -type f -iname '*Suspiro*.wav' -exec mv {} mestis-raw/ \;
find . -maxdepth 1 -mindepth 1 -type f -name '*.wav' -exec rm {} \;

mkdir -p mestis-processed
mkdir -p periphery-processed

echo "Processing each wav file to 16kHz mono"

for f in mestis-raw/*.wav; do
        ffmpeg -i "${f}" -ac 1 -ar 16000 "mestis-processed/$(basename "$f")";
done

for f in periphery-raw/*.wav; do
        ffmpeg -i "${f}" -ac 1 -ar 16000 "periphery-processed/$(basename "$f")";
done

mkdir -p periphery-chunks
mkdir -p mestis-chunks
mkdir -p mixed-chunks

for f in mestis-processed/*.wav; do
        python ../chunk_audio.py --input_file "${f}" --output_dir mestis-chunks --chunk_length 8000 --overlap 1000
        python ../chunk_audio.py --input_file "${f}" --output_dir mixed-chunks --chunk_length 8000 --overlap 1000
done

for f in periphery-processed/*.wav; do
        python ../chunk_audio.py --input_file "${f}" --output_dir periphery-chunks --chunk_length 8000 --overlap 1000
        python ../chunk_audio.py --input_file "${f}" --output_dir mixed-chunks --chunk_length 8000 --overlap 1000
done
    </pre>
    What the script does is:
    <ul>
	    <li>Fetch files for every Mestis song (from YouTube playlists + youtube-dl)</li>
	    <li>Fetch files for every instrumental Periphery song (from YouTube playlists + youtube-dl)</li>
	    <li>Pre-process them into 16kHz mono with ffmpeg (for optimal training)</li>
	    <li>Apply the chunk_audio.py script to split into non-silent 8-second chunks with 1 second overlap</li>
	    <li>Create 3 sets of training data - periphery-chunks, mestis-chunks, mixed-chunks</li>
    </ul>
    My intention was to train the model on each of the sets of training chunks, to create generated music that:
    <ul>
	    <li>Sounds like Periphery only</li>
	    <li>Sounds like Mestis only</li>
	    <li>Sounds like "Mestiphery", an organic mashup</li>
    </ul>
    <h2>Experiment 1 part 1 - Periphery only</h2>
    The training command for periphery-only is:
    <pre>
$ python train.py --id periphery_only --data_dir ./experiment-1/periphery-chunks/ --num_epochs 100 --batch_size 64 --sample_rate 16000
    </pre>
    It took <b>1 week</b> to complete the training on Periphery only. I bold 1 week because, in an academic semester of 3 months, taking 8% of the semester to run one single experiment in a large project concerns me. My limitation is computational power. I could have taken advantage of expensive hardware (e.g. using the resources of my future research lab, DDMAL), but I felt that this would go against the spirit of an independent student project.
    <br>
    <br>
    Also, the conclusion that running all of my envisioned experiments is impossible in the time frame of a semester is actually a valuable outcome. It shows the difficulty of black-box computational models where the large companies or labs publishing papers are leaps and bounds ahead of casual readers due to the unprecedented compute power available to them(FIND REFERENCE FAANGMAN NEURAL NETWORKS).
    <h3>Periphery only results</h3>
    Here's a 30-second clip output from the training on Periphery only:
    <br>
    <audio controls>
	    <source src="periphery_only_0.wav" type="audio/wav"</source>
    </audio>
    <br>
    <br>
    Another disappointing trait is that most generated audio consists of silence. I was very lucky to get almost 30 seconds of musical content in a single clip. Subjectively, this sounds nothing like Periphery:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/1_VZji3YFo4?start=90" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    This nullifies my hypothesis, that it's possible to expect a neural network trained (by an amateur) for 1 week to sound like a band of real professional musicians. A trend I witnessed in all of the Periphery generated clips is the lack of palm-muted distorted guitar chords (a sound very characteristic to Periphery) in the final outputs. All of the harmonic content seems to be a high-pitched whistling, but there are convincing drum sounds:
    <br>
    <br>
    Epoch 67:
    <br>
    <audio controls>
	    <source src="periphery_only_epoch_67.wav" type="audio/wav"</source>
    </audio>
    <br>
    Epoch 70:
    <br>
    <audio controls>
	    <source src="periphery_only_epoch_70.wav" type="audio/wav"</source>
    </audio>
    <br>

    <h3>Generated audio visualization</h3>
    The above clips show collections of realistic note onsets. One of my original statements about WaveNet and SampleRNN was that they could produce music with convincing dynamics and timbre, to make us believe real humans played it. Observing various aspects of the waveform in the time and frequency domain should be useful.
    <br>
    <br>
    Let's view each clip (epochs 67, 70) in the time domain and frequency domain (with a spectrogram):
    <img src="periphery_epoch_67_timedomain.png" width=800px/>
    <img src="periphery_epoch_70_timedomain.png" width=800px/>
    <img src="periphery_epoch_67_spectrogram.png" width=800px/>
    <img src="periphery_epoch_70_spectrogram.png" width=800px/>
    <br>
    Although this is subjective, one can see the dynamic nature of the produced audio in the plots above. It really does look like there are real musical variations in the complex waveform (aside from the totally blank silences which are odd in real music).

    <h3>Mu-law vs linear quantization</h3>
    Karl Hiner's blog post touches on WaveNet's mu-law quantization, and claims it sounds better than SampleRNN's linear quantization. In fact, I found that every SampleRNN implementation I found had options for linear and mu-law quantization (perhaps it was added later). In fact the original ICLR 2017 paper even has an "a-law quantization" (similar to mu-law). Let's hear what each sounds like:
    <br>
    <br>
    Periphery epoch 83, mu-law quantization:
    <br>
    <audio controls>
	    <source src="periphery_epoch_83_mulaw.wav" type="audio/wav"</source>
    </audio>
    <br>
    Periphery epoch 83, linear quantization <b>WARNING! LOUD!</b>:
    <br>
    <audio controls>
	    <source src="periphery_epoch_83_linear.wav" type="audio/wav"</source>
    </audio>
    <br>
    In my subjective listening test, the linear quantization output is very loud, almost to the point of distortion and clipping. The mu-law quantization is outputting music with more subtle volume, possibly since its more suited for the logarithmic human experience of loudness.

    <h2>Rolling experiment 2+ into experiment 1</h2>
    As noted, experiment 1 took <b>1 week</b> to train, or 8% of the Fall semester. To top it off, the results were very divorced from my expectations:
    <ul>
      <li>The resulting generated audio is mostly silence and junk</li>
      <li>There are some interesting potentially musical sounds, but it's all high-pitched whistling and doesn't contain any characteristics of the band Periphery (palm-muted distorted guitar chords, etc.)</li>
      <li>I have to generate 100s or 1000s of clips and curate the results to create a final result - the chances of getting 1 cohesive "song" (let's say, 2 minutes of contiguous music) are pretty slim</li>
    </ul>
    With this in mind, I didn't see any value in performing training on separate bands, and I didn't think I'd have enough time to train on separate instruments or song segments. Instead, what I did was mix the Mestis data into the Periphery data, and continue training the model (that was initially trained only on Periphery for 100 epochs) for 250 epochs.
    <br>
    <br>
    This took another week, bringing the sum total of all generative experiments (including failed starts) to <b>3 weeks</b>. 3 weeks of letting my computer crunch data to compute a model that produces strange, confusing, and mostly silent outputs was a disappointing experience.
    <br>
    <br>
    In the subsequent generation + curation phases, I'll use different model checkpoints (between epochs 0 and 250) to generate music at different points of the model's knowledge.
    <h2>Training results</h2>
    The results of training SampleRNN are stored in the <code>logdir</code> directory, in timestamped directories storing checkpoints for intermediate epochs in training. Here's a <code>tree</code> view of of the training. The size of the training directory is 51GB, after having been trained on all of the albums listed above (709MB of music):
    <pre>
$ tree logdir/periphery_only/ -L 2
logdir/periphery_only/
├── 15.09.2020_17.36.46
│   ├── checkpoint
│   ├── model.ckpt-79.data-00000-of-00001
│   ├── model.ckpt-79.index
│   ├── model.ckpt-80.data-00000-of-00001
│   ├── model.ckpt-80.index
│   ├── model.ckpt-81.data-00000-of-00001
│   ├── model.ckpt-81.index
│   ├── model.ckpt-82.data-00000-of-00001
│   ├── model.ckpt-82.index
│   ├── model.ckpt-83.data-00000-of-00001
│   ├── model.ckpt-83.index
│   └── train
├── 17.09.2020_21.55.43
│   ├── checkpoint
│   ├── model.ckpt-89.data-00000-of-00001
│   ├── model.ckpt-89.index
│   ├── model.ckpt-90.data-00000-of-00001
│   ├── model.ckpt-90.index
│   ├── model.ckpt-91.data-00000-of-00001
│   ├── model.ckpt-91.index
│   ├── model.ckpt-92.data-00000-of-00001
│   ├── model.ckpt-92.index
│   ├── model.ckpt-93.data-00000-of-00001
│   ├── model.ckpt-93.index
│   └── train
...truncated...
└── 22.09.2020_22.25.13
    ├── checkpoint
    ├── model.ckpt-244.data-00000-of-00001
    ├── model.ckpt-244.index
    ├── model.ckpt-245.data-00000-of-00001
    ├── model.ckpt-245.index
    ├── model.ckpt-246.data-00000-of-00001
    ├── model.ckpt-246.index
    ├── model.ckpt-247.data-00000-of-00001
    ├── model.ckpt-247.index
    ├── model.ckpt-248.data-00000-of-00001
    ├── model.ckpt-248.index
    └── train
    </pre>
    <h1>Salvaging neural audio results with curation</h1>
    An important thing to note is that the Dadabots never claimed that their model spit out perfect music. They focus heavily on curating<sup><a href="#ref">[16]</a></sup> the resultant audio clips. From a blackbox neural network that produces any kind of audio, from silence, to a cacophony of sounds, and everything in between, one needs to curate the results to combine them into a cohesive piece of music without long periods of silence.
    <br>
    <br>
    The goal of my automated curation script is to:
    <ul>
	    <li>Load every clip generated by the trained SampleRNN model</li>
	    <li>Trim out the silence</li>
	    <li>Potentially apply some MIR techniques* to group similar clips together, to create the sense of a "cohesive" musical piece</li>
	    <li>Concatenate the result clips of sound with crossfading</li>
    </ul>
    *Ideally, we would use popular MIR Python libraries (librosa, madmom, Essentia) to assist us in the curation step, e.g. by grouping similar clips by their musical content. However, as this is more in the domain of MUMT 621 (music information retrieval), and not an easy task, I won't spend too much time on it.
    <br>
    <br>
    I want to minimize the amount of visual/manual audio curation that I do, and rely only on the simple curation listed above.

    <h2>Generating music with the trained model</h2>
    First, I use the trained model described earlier to generate audio clips at various epochs. I chose the epochs randomly - the ones before checkpoint 100 are trained only on Periphery, while the ones after checkpoint 100 are trained on a mix of Periphery and Mestis (aka "Mestiphery").
    <br>
    <br>
    According to the dadabots' README<sup><a href="#ref">[18]</a></sup>, they don't necessarily accept that the latest training epoch is the best one. That's why I picked a variety of epochs:
    <blockquote>
      However, we found the latest checkpoint does not always create the best music. Instead we listen to the test audio generated at each checkpoint, choose our favorite checkpoint, and delete the newer checkpoints, before generating a huge batch with this script.
    </blockquote>
    I generated sequences of different durations. The generate commands are as follows:
    <pre>
# epoch 83, 92 (periphery only)

$ python generate.py --output_path ./to-curate-periphery-epoch-83/gen.wav --checkpoint_path ./logdir/periphery_only/15.09.2020_17.36.46/model.ckpt-83 --config_file ./default.config.json --num_seqs 10 --dur 30 --sample_rate 16000
$ python generate.py --output_path ./to-curate-periphery-epoch-83-2/gen.wav --checkpoint_path ./logdir/periphery_only/15.09.2020_17.36.46/model.ckpt-83 --config_file ./default.config.json --num_seqs 100 --dur 8 --sample_rate 16000
$ python generate.py --output_path ./to-curate-periphery-epoch-92/gen.wav --checkpoint_path ./logdir/periphery_only/17.09.2020_21.55.43/model.ckpt-92 --config_file ./default.config.json --num_seqs 10 --dur 30 --sample_rate 16000

# epoch 139, 165, 188 (mestiphery - mixed periphery + mestis)

$ python generate.py --output_path ./to-curate-mestiphery-epoch-139/gen.wav --checkpoint_path ./logdir/periphery_only/18.09.2020_04.32.49/model.ckpt-139 --config_file ./default.config.json --num_seqs 10 --dur 30 --sample_rate 16000
$ python generate.py --output_path ./to-curate-mestiphery-epoch-165/gen.wav --checkpoint_path ./logdir/periphery_only/20.09.2020_10.04.13/model.ckpt-165 --config_file ./default.config.json --num_seqs 100 --dur 8 --sample_rate 16000
$ python generate.py --output_path ./mestiphery-to-curate-epoch-188/gen.wav --checkpoint_path ./logdir/periphery_only/20.09.2020_17.12.32/model.ckpt-188 --config_file ./default.config.json --num_seqs 30 --dur 10 --sample_rate 16000
    </pre>
    The results of this training are directories named "to-curate-*" full of wav files:
    <pre>
$ ls to-curate-*
to-curate-mestiphery-epoch-139:
gen_0.wav  gen_1.wav  gen_2.wav  gen_3.wav  gen_4.wav  gen_5.wav  gen_6.wav  gen_7.wav  gen_8.wav  gen_9.wav

to-curate-mestiphery-epoch-165:
gen_0.wav   gen_28.wav  gen_46.wav  gen_64.wav  gen_82.wav
gen_10.wav  gen_29.wav  gen_47.wav  gen_65.wav  gen_83.wav
...truncated...

to-curate-mestiphery-epoch-188:
gen_0.wav   gen_12.wav  gen_15.wav  gen_18.wav  gen_20.wav  gen_23.wav  gen_26.wav  gen_29.wav  gen_4.wav  gen_7.wav
gen_10.wav  gen_13.wav  gen_16.wav  gen_19.wav  gen_21.wav  gen_24.wav  gen_27.wav  gen_2.wav   gen_5.wav  gen_8.wav
gen_11.wav  gen_14.wav  gen_17.wav  gen_1.wav   gen_22.wav  gen_25.wav  gen_28.wav  gen_3.wav   gen_6.wav  gen_9.wav

to-curate-mestiphery-epoch-92:
gen_0.wav  gen_1.wav  gen_2.wav  gen_3.wav  gen_4.wav  gen_5.wav  gen_6.wav  gen_7.wav  gen_8.wav  gen_9.wav

to-curate-periphery-epoch-83:
gen_0.wav  gen_2.wav  gen_4.wav  gen_6.wav  gen_8.wav
gen_1.wav  gen_3.wav  gen_5.wav  gen_7.wav  gen_9.wav

to-curate-periphery-epoch-83-2:
gen_0.wav   gen_28.wav  gen_46.wav  gen_64.wav  gen_82.wav
gen_10.wav  gen_29.wav  gen_47.wav  gen_65.wav  gen_83.wav
gen_11.wav  gen_2.wav   gen_48.wav  gen_66.wav  gen_84.wav
    </pre>
    After listening to several of the files manually, I noticed that the lower epochs produce better, more cohesive, and longer bits of music. This to me indicates the failure of experiment 2, of mixing Periphery + Mestis and training for 250 epochs. This could be due to the nature of the training data - the neural network is trying to generate samples that minimize error for two distinct musical artists and styles, so it can resort to outputting nothing (to minimize numerical error between the two sets of training data). Also, like the Dadabots say, perhaps training for too long isn't beneficial.
    <h2>Chromaprint-based curation script</h2>
    <h1>Supplementing neural audio with structured synthesis</h1>
    <h2>Magenta and Differentiable DSP</h2>
    <h2>Training Karplus-Strong parameters with machine learning</h2>
    <h2>Creating some pieces of synthesized music with trained Karplus-Strong + algorithmic music generation</h2>
    <h1>Album art generation</h1>
    <h1>Putting it all together</h1>
    <h1>Final result</h1>
    <h1>Conclusion</h1>

    <h1 id="ref">References</h2>
    <ol>
      <li>mlacoustics (citations.bib)</li>
      <li>wavenetpaper (citations.bib)</li>
      <li>samplernnpaper (citations.bib)</li>
      <li>magentaddsp (citations.bib)</li>
      <li>dadabots (citations.bib)</li>
      <li>new ref: machine learning reproducability crisis https://petewarden.com/2018/03/19/the-machine-learning-reproducibility-crisis</li>
      <li>prism (citations.bib)</li>
      <li>prism-blog (citations.bib)</li>
      <li>prism gh commits</li>
      <li>my fork</li>
      <li>tf weird fix https://github.com/search?q=%22tf.config.experimental.set_memory_growth%28physical_devices%5B0%5D%2C+enable%3DTrue%29%22&type=issues</li>
      <li> conda </li>
      <li> dadabots curation </li>
    </ol>
  </body>
</html>

