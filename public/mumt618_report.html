<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="GitLab Pages">
    <title>1000 sharks - an AI music experience</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body id="index">
    <div class="navbar">
      <a href="./mumt618_report.html">MUMT 618 report</a>
      <a href="./">1000sharks demo</a>
    </div>

    <h1>1000 sharks</h1>
    <h3>An AI music experience, prepared by Sevag for MUMT 618, Fall 2020</h3>

    <p>
    Machine learning and deep learning techniques in recent years have an increasing presence in the field of musical acoustics<sup><a href="#ref">[1]</a></sup>. As a preliminary introduction to generating music using neural networks, you can view my <a href="paper_presentation_final.pdf" type="application/pdf" title="Neural audio synthesis">paper presentation</a> introducing WaveNet<sup><a href="#ref">[2]</a></sup>, SampleRNN<sup><a href="#ref">[3]</a></sup>, and other novel approaches to neural audio.
    <br>
    <br>
    In the presentation, I outlined that there exist both unconstrained, unstructured models for generating audio in the waveform domain (resulting in babbling or confusing outputs), and structured physical computational models that have been used in traditional audio synthesis (that don't sound very natural).
    <br>
    <br>
    In this project, I'll use SampleRNN for unstructured neural audio synthesis. From this diagram, I'll be targeting the top left image:
    <figure>
    <img src="6_5_ml_acoustics.jpg" width=400px/>
    <figcaption>By augmenting ML methods (top left) with physical models (bottom right) to obtain hybrid models (upper right), a synergy of the strengths of physical intuition and data-driven insights can be obtained <sup><a href="#ref">[1]</a></sup></figcaption>
    </figure>
    </p>

    <h1>Motivation</h1>
    <p>
    The dadabots<sup><a href="#ref">[4]</a></sup> have been creating music with neural networks, trained on the music of specific artists:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/MwtVkPKx3RA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    My idea is to imitate the dadabots by training SampleRNN to create original heavy metal music. To supplement the illusion of a "real" music artist, I also want to generate album art using additional AI techniques; as these are not directly related to musical acoustics, they'll occupy a small section at the bottom of this report.
    <br>
    <br>
    To explain the overall aesthetic/theme of death metal + sharks:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/EzCbsw3WgTk?start=85" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <h1>State of the art in neural audio</h1>
    <a href="https://arxiv.org/abs/1609.03499">WaveNet</a> in 2016 and <a href="https://arxiv.org/abs/1612.07837">SampleRNN</a> in 2017 are both neural network architecture for unconditional audio generation in the waveform domain. These differ from traditional audio synthesis techniques which use the spectral or symbolic domain(timbre 2018). By modelling the waveform directly, the phase is preserved implicitly, leading to high quality, realistic generated audio. The downsides are the black box/opaque models, and the unstructured, babbling outputs. These models are also computationally expensive, since waveforms require a lot of samples to represent recognizeable speech or music features.
    <br>
    <br>
    Several interesting papers build on WaveNet. Two that I mentioned in the paper presentation and will analyze as part of this project are:
    <ul>
      <li><i>Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions</i> - <a href="https://arxiv.org/abs/1712.05884">Tacotron 2</a>, Google's next-gen Text-to-Speech model based on WaveNet in 2017</li>
      <li><i>Jukebox: A Generative Model for Music</i> - <a href="https://arxiv.org/abs/2005.00341">Jukebox</a>, a coherent music + vocal generation model in 2020</li>
    </ul>
    Other derivative works (that may be interesting but I won't be talking about) are:
    <ul>
      <li><a href="https://www.researchgate.net/publication/329206913_TimbreTron_A_WaveNetCycleGANCQTAudio_Pipeline_for_Musical_Timbre_Transfer">TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer</a></li>
      <li><a href="https://arxiv.org/abs/1802.08435">WaveRNN</a>, Efficient Neural Audio Synthesis</li>
      <li><i>Real-Time Guitar Amplifier Emulation with Deep Learning</i> - <a href="https://www.mdpi.com/2076-3417/10/3/766/htm">WaveNetVA</a> (virtual-analog) model for effects modelling in 2019</li>
    </ul>
    It'll be useful to dig a little deeper under the surface of both WaveNet and SampleRNN to gain an intuition of how they work, and to also look at Tacotron 2 and Jukebox to see how they build on WaveNet.

    <h2>Deeper dives</h2>
    Despite the "black box" nature of WaveNet and SampleRNN, we can do a below-the-surface examination of the models (including training and generation steps) to get a better sense of:
    <ol>
	    <li>Input data and preprocessing - how are the training waveforms represented?</li>
	    <li>The model/neural network itself - what are WaveNet and SampleRNN computing?</li>
	    <li>Loss function and iterative training/optimization - how do WaveNet/SampleRNN know that one set of parameters is better than the other? What defines the "correct" output of a waveform?</li>
	    <li>Generation - after training a model with low loss, how do WaveNet and SampleRNN use the trained model to generate brand new waveforms of audio?</li>
    </ol>
    <a href="/wavenet_overview.html"><h3>WaveNet overview</h3></a>
    <a href="/samplernn_overview.html"><h3>SampleRNN overview</h3></a>
    <a href="/wavenet_derivatives.html"><h3>WaveNet derivatives - Tacotron 2 and Jukebox</h3></a>

    <h1>Experiments</h1>
    A difficult step in any machine learning experiment is actually reproducing the claims of papers<sup><a href="#ref">[5]</a></sup>. I'll run experiments with 2-tier and 3-tier SampleRNN to verify whether I can get good results.

    <h2>Hardware and OS setup</h2>

    I ran all of the training and experiments on my personal desktop computer, consisting of:
    <ul>
      <li>AMD Ryzen 7 3700X 8c/16t processor</li>
      <li>32GB RAM</li>
      <li>1TB NVME storage</li>
      <li>NVIDIA RTX 2070 SUPER GPU</li>
      <li>Fedora 32 OS</li>
    </ul>

    <a href="./samplernn_3tier_experiments.html"><h2>3-tier SampleRNN experiments</h2></a>

    <a href="./samplernn_2tier_experiments.html"><h2>2-tier SampleRNN experiments</h2></a>

    <h2>(tentative) WaveNet experiments</h2></a>

    This is an optional part of the project, since SampleRNN is considered better at generating music. It may still be instructive to get an idea of how to train WaveNet and to verify some results.
    
    <a href="./curation.html"><h1>Curating neural generated results</h1></a>

    <a href="./album_art.html"><h1>Album art generation</h1></a>

    <h1>Results & conclusion</h1>

    Machine learning and modern AI has problems with unexplained results, incomplete models and unreproducable results<sup><a href="#ref">[6]</a></sup>. Some excerpts from the reviewers of that paper<sup><a href="#ref">[7]</a></sup>:
    <blockquote>
    High reproducibility might be difficult with some of the resource-intensive empirical papers within corporate labs where huge clusters and massive GPUs are used to run experiments. Running the code on smaller-scale data would not be reproducible because some of the algorithmic improvements show up only at scale.
    </blockquote>
    and
    <blockquote>
    issue can come up with any scientific reporting if researchers try many different combinations and "clean up" the writing to report only the experimental configuration that worked. This can include subtle code parameters and specific datasets that support improvements in a paper. Even high reproducibility does not prevent researchers from leading the research reproducer through only those experimental steps that support the claims made in the research.
    </blockquote>
    Throughout the course of this project, I didn't encounter the first problem (lack of computing resources), although this is especially relevant today with problems of "AI democratization"<sup><a href="#ref">[8]</a></sup>. For the most part, I was able to train the models on my personal computer and get results within several days of training. It was definitely a frustrating development process (compared to the immediate feedback loop of writing regular code).
    <br>
    <br>
    I believe I encountered more of the second issue. WaveNet and SampleRNN are presented as if they're obvious, but as a beginner to machine learning, I didn't find the choices obvious at all. For example, where did Wavenet get its default value for dilations? How did SampleRNN end up with its default number of layers per RNN? The academic papers in both cases are concise but are seemingly intended for machine learning practitioners.
    <br>
    <br>
    Here's what I accomplished with this project:
    <ul>
      <li>I explored WaveNet and SampleRNN to try to explain away some of the opacity and present a combination of paper + code overviews to find out how these models learn how to create music. The overviews aren't as in-depth as I would have liked (as I couldn't understand the entirety of the model implementations), but hopefully a reader will get a sense of the intuition behind these models.</li>
      <li>I was able to recreate the dadabots AI-generated death metal successfully by using their 2-tier SampleRNN model. I created a convincing song that sounded like the artist Cannibal Corpse by overfitting to one of their albums.</li>
      <li>I confirmed that 2-tier SampleRNN makes better music than 3-tier (as reported by the paper author and the dadabots)</li>
      <li>I wrote a curation script that could combine the SampleRNN results into a (mostly) cohesive song, without the hours of manual curation described by the dadabots.
      <li>I was able to set up, train, and get results from a diverse set of machine learning models, all based on different version of Python and different machine learning libraries. I included snippets of my setup instructions, as well any necessary code tweaks that one would need to reproduce my experiments.</li>
    </ul>
    The final result is <a href="./">here</a>. The source code for this website can be seen <a href="https://gitlab.com/sevagh/1000sharks.xyz">here</a>.
    <h2>Future work</h2>
    Future work can include:
    <ul>
      <li>Modify the PRiSM SampleRNN code, which is modern and clean, to add the 2-tier implementation - benefit from the new codebase and the better model for music</li>
      <li>Using more advanced models to create more structured outputs - the current state of the art seems to be Jukebox<sup><a href="#ref">[9]</a></sup></li>
      <li>Build a derivative of WaveNet or SampleRNN for a more specific task (e.g. the amp modeling shown above), to demonstrate the extensibility of sample-based/waveform domain machine learning models</sup></li>
    </ul>

    <h1 id="ref">References</h2>
    <ol>
      <li><a href="https://asa.scitation.org/doi/10.1121/1.5133944">Machine learning in acoustics: Theory and applications: The Journal of the Acoustical Society of America: Vol 146, No 5</a></li>
      <li><a href="https://arxiv.org/abs/1609.03499">WaveNet: A Generative Model for Raw Audio - arXiv.org</a></li>
      <li><a href="https://arxiv.org/abs/1612.07837">SampleRNN: An Unconditional End-to-End Neural Audio Generation Model - arXiv.org</a></li>
      <li><a href="https://arxiv.org/abs/1811.06633">Generating Albums with SampleRNN to Imitate Metal, Rock, and Punk Bands - arXiv.org</a></li>
      <li><a href="https://petewarden.com/2018/03/19/the-machine-learning-reproducibility-crisis/">The Machine Learning Reproducability Crisis</a></sup>
      <li><a href="https://openreview.net/pdf/590d904c6dbdcad65fbc7f422b839c4822c07f60.pdf">A Practical Taxonomy of Reproducibility for Machine Learning Research - paper</a></sup>
      <li><a href="https://openreview.net/forum?id=B1eYYK5QgX">A Practical Taxonomy of Reproducibility for Machine Learning Research - OpenReview.org review comments</a></sup>
      <li><a href="https://thegradient.pub/ai-democratization-in-the-era-of-gpt-3/">AI Democratization in Era of GPT-3</a></sup>
      <li><a href="https://openai.com/blog/jukebox/">Jukebox by OpenAI</a></sup>
    </ol>
  </body>
</html>
