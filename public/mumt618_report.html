<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="GitLab Pages">
    <title>1000 sharks - an AI music experience</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body id="index">
    <div class="navbar">
      <a href="./mumt618_report.html">MUMT 618 report</a>
      <a href="./">1000sharks demo</a>
    </div>

    <h1>1000 sharks</h1>
    <h3>An AI music experience, prepared by Sevag for MUMT 618, Fall 2020</h3>

    <p>
    Machine learning and deep learning techniques in recent years have an increasing presence in the field of musical acoustics<sup><a href="#ref">[1]</a></sup>. As a preliminary introduction to generating music using neural networks, you can view my <a href="paper_presentation_final.pdf" type="application/pdf" title="Neural audio synthesis">paper presentation</a> introducing WaveNet<sup><a href="#ref">[2]</a></sup>, SampleRNN<sup><a href="#ref">[3]</a></sup>, and other novel approaches to neural audio:
    <object data="paper_presentation_final.pdf" title="Neural audio synthesis" width="700px" height="512px">
    </object>
    <br>
    <br>
    In the presentation, I outlined that there exist both unconstrained, unstructured models for generating audio in the waveform domain (resulting in babbling or confusing outputs), and structured physical computational models that have been used in traditional audio synthesis (that don't sound very natural).
    <br>
    <br>
    In this project, I'll analyze WaveNet, SampleRNN (which is a homegrown Montreal project!), Jukebox for pure machine learning/neural audio synthesis approaches, and Magenta DDSP (a collection of differentiable DSP building blocks e.g. sinusoidal oscillators, FIR filters) as a hybrid machine learning/physical modelling approach:
    <figure>
    <img src="6_5_ml_acoustics.png" width=500px/>
    <figcaption>By augmenting ML methods (top left) with physical models (bottom right) to obtain hybrid models (upper right), a synergy of the strengths of physical intuition and data-driven insights can be obtained <sup><a href="#ref">[1]</a></sup></figcaption>
    </figure>
    </p>

    <h1>Motivation</h1>
    <p>
    The dadabots<sup><a href="#ref">[4]</a></sup> have been creating music with neural networks, trained on the music of specific artists:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/MwtVkPKx3RA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    My idea is to imitate the dadabots by training SampleRNN to create original heavy metal music. To supplement the illusion of a "real" music artist, I also want to generate album art using additional AI techniques; as these are not directly related to musical acoustics, they'll occupy a small section at the bottom of this report.
    <br>
    <br>
    To explain the overall aesthetic/theme of death metal + sharks:
    <br>
    <iframe width="300" height="256" src="https://www.youtube.com/embed/EzCbsw3WgTk?start=85" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <h1>State of the art in neural audio</h1>
    <a href="https://arxiv.org/abs/1609.03499">WaveNet</a> in 2016 and <a href="https://arxiv.org/abs/1612.07837">SampleRNN</a> in 2017 are both neural network architecture for unconditional audio generation in the waveform domain. These differ from traditional audio synthesis techniques which use the spectral or symbolic domain<sup><a href="#ref">[5]</a></sup>. By modelling the waveform directly, the phase is preserved implicitly, leading to high quality, realistic generated audio. The downsides are the black box/opaque models, and the unstructured, babbling outputs. These models are also computationally expensive, since waveforms require a lot of samples to represent recognizeable speech or music features.
    <br>
    <br>
    Several interesting papers build on WaveNet. I will analyze a few as part of this project, for illuminating some important aspects or modifications about WaveNet and SampleRNN:
    <ul>
      <li><i>Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions</i> - <a href="https://arxiv.org/abs/1712.05884">Tacotron 2</a>, Google's new (2017) Text-to-Speech model with a WaveNet vocoder</li>
      <li><i>Jukebox: A Generative Model for Music</i> - <a href="https://arxiv.org/abs/2005.00341">Jukebox</a>, a coherent music + vocal generation model in 2020</li>
      <li><i>Char2Wav: End-to-End Speech Synthesis</i> - <a href="http://josesotelo.com/speechsynthesis/">Char2Wav</a>, a 2017 speech synthesizer with a SampleRNN vocoder</li>
    </ul>
    Other derivative works (that may be interesting but I won't be talking about) are:
    <ul>
      <li><a href="https://www.researchgate.net/publication/329206913_TimbreTron_A_WaveNetCycleGANCQTAudio_Pipeline_for_Musical_Timbre_Transfer">TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer</a></li>
      <li><a href="https://arxiv.org/abs/1802.08435">WaveRNN</a>, Efficient Neural Audio Synthesis</li>
      <li><i>Real-Time Guitar Amplifier Emulation with Deep Learning</i> - <a href="https://www.mdpi.com/2076-3417/10/3/766/htm">WaveNetVA</a> (virtual-analog) model for effects modelling in 2019</li>
    </ul>
    It'll be useful to dig a little deeper under the surface of both WaveNet and SampleRNN to gain an intuition of how they work, and to also look at Tacotron 2 and Jukebox to see how they build on WaveNet.

    <h2>Deeper dives</h2>
    Despite the "black box" nature of WaveNet and SampleRNN, we can do a below-the-surface examination of the models (including training and generation steps) to get a better sense of:
    <ol>
	    <li>Input data and preprocessing - how are the training waveforms represented?</li>
	    <li>The model/neural network itself - what are WaveNet and SampleRNN computing?</li>
	    <li>Loss function and iterative training/optimization - how do WaveNet/SampleRNN know that one set of parameters is better than the other? What defines the "correct" output of a waveform?</li>
	    <li>Generation - after training a model with low loss, how do WaveNet and SampleRNN use the trained model to generate brand new waveforms of audio?</li>
    </ol>
    Individual deep dive sub-pages per model:
    <ul>
	    <li><a href="/wavenet_overview.html"><b>WaveNet overview</b></a></li>
    	    <li><a href="/samplernn_overview.html"><b>SampleRNN overview</b></a></li>
    	    <li><a href="/wavenet_derivatives.html"><b>WaveNet and SampleRNN derivatives - Tacotron 2, Jukebox, Char2Wav</b></a></li>
    	    <li><a href="/hybrid.html"><b>ML/DSP hybrids - Magenta DDSP, RNNoise</b></a></li>
    </ul>

    <h1>Training the models</h1>
    A difficult step in any machine learning experiment is actually reproducing the claims of papers<sup><a href="#ref">[6]</a></sup>. I'll try to reproduce some of the claims of the described projects and models.

    <h2>Hardware and OS setup</h2>

    I ran all of the training and experiments on my personal desktop computer, consisting of:
    <ul>
      <li>AMD Ryzen 7 3700X 8c/16t processor</li>
      <li>32GB RAM</li>
      <li>1TB NVME storage</li>
      <li>NVIDIA RTX 2070 SUPER GPU</li>
      <li>Fedora 32 OS</li>
    </ul>
    The most important component by far in GPU-based deep learning is the GPU. For comparison<sup><a href="#ref">[7]</a></sup>:
    <ul>
	    <li>The dadabots used a Tesla V100<sup><a href="#ref">[8]</a></sup>, which is 2x better than my RTX 2070 SUPER</li>
	    <li>The authors of SampleRNN used a K80<sup><a href="#ref">[9]</a></sup>, which 2 K40s on one die - this one is actually worse than my RTX 2070 SUPER, but there are 2 of them</li>
	    <li>The authors of Jukebox used up to 512 Tesla V100s training for multiple weeks</li>
    </ul>

    <h2>Experiments</h2>
    I used the default parameters and instructions for the 3 variants - WaveNet, prism/3-tier SampleRNN, dadabots/2-tier SampleRNN, and DDSP. My goal was to gain a minimum amount of experience in the real world usage of the models after doing a theoretical description/overview earlier.
    <br>
    <br>
    Experiment sub-pages per model:
    <ul>
	    <li><a href="./prism_samplernn.html"><b>prism-samplernn experiments (3-tier)</b></a></li>
    	    <li><a href="./dadabots_samplernn.html"><b>dadabots_sampleRNN experiments (2-tier)</b></a></li>
    	    <li><a href="./samplernn_bridging_the_gap.html"><b>Bridging the gap between SampleRNN implementations</b></a></li>
    	    <li><a href="./wavenet_experiments.html"><b>WaveNet experiments</b></a></li>
    	    <li><a href="./ddsp_experiments.html"><b>DDSP experiments - clarinet synthesis</b></a></li>
    	    <li><a href="./album_art.html"><b>Album art generation with StyleGAN2</b></a></li>
    </ul>

    <h1>Curating neural results</h1>
    After generating disjoint clips with varying amounts of musical content with the above neural network models, these need to be curated to form larger "songs" - I do this with an automated <a href="./curation.html">curation script</a>.

    <h1>Results & conclusion</h1>

    Machine learning and modern AI has problems with unexplained results, incomplete models and unreproducable results<sup><a href="#ref">[10]</a></sup>. Some excerpts from the reviewers of that paper<sup><a href="#ref">[11]</a></sup>:
    <blockquote>
    High reproducibility might be difficult with some of the resource-intensive empirical papers within corporate labs where huge clusters and massive GPUs are used to run experiments. Running the code on smaller-scale data would not be reproducible because some of the algorithmic improvements show up only at scale.
    </blockquote>
    and
    <blockquote>
    issue can come up with any scientific reporting if researchers try many different combinations and "clean up" the writing to report only the experimental configuration that worked. This can include subtle code parameters and specific datasets that support improvements in a paper. Even high reproducibility does not prevent researchers from leading the research reproducer through only those experimental steps that support the claims made in the research.
    </blockquote>
    Throughout the course of this project, I didn't encounter the first problem (lack of computing resources), although this is especially relevant today with problems of "AI democratization"<sup><a href="#ref">[12]</a></sup>. For the most part, I was able to train the models on my personal computer and get results within several days of training. It was definitely a frustrating development process (compared to the immediate feedback loop of writing regular code).
    <br>
    <br>
    I believe I encountered more of the second issue. WaveNet and SampleRNN are presented as if they're obvious, but as a beginner to machine learning, I didn't find the choices obvious at all. For example, where did Wavenet get its default value for dilations? How did SampleRNN end up with its default number of layers per RNN? The academic papers in both cases are concise but are seemingly intended for machine learning practitioners.
    <br>
    <br>
    Here's what I accomplished with this project:
    <ul>
      <li>I explored WaveNet and SampleRNN to try to explain away some of the opacity and present a combination of paper + code overviews to find out how these models learn how to create music. The overviews aren't as in-depth as I would have liked (as I couldn't understand the entirety of the model implementations), but hopefully a reader will get a sense of the intuition behind these models.</li>
      <li>I was able to recreate the dadabots AI-generated metal successfully by using their 2-tier SampleRNN model. I created songs that sounded like the artist Cannibal Corpse and Animals as Leaders by overfitting to their albums.</li>
      <li>I modified the PRiSM-SampleRNN (better code quality) implementation to produce better music</li>
      <li>I wrote a curation script that could combine the SampleRNN results into a (mostly) cohesive song, without the hours of manual curation described by the dadabots.
      <li>I was able to set up, train, and get results from a diverse set of machine learning models, all based on different version of Python and different machine learning libraries. I included snippets of my setup instructions, as well any necessary code tweaks that one would need to reproduce my experiments.</li>
    </ul>
    The final result is <a href="./">here</a>. The source code for this website can be seen <a href="https://gitlab.com/sevagh/1000sharks.xyz">here</a>.
    <h2>Future work</h2>
    Future work can include:
    <ul>
      <li>Follow a cleaner experimentation process, i.e. strictly recording training/generation time and testing various hypotheses - e.g. increasing or decreasing the receptive field of WaveNet and verifying the results. More easily done on a GPU cluster, not a personal computer.</li>
      <li>Using more advanced models to create more structured outputs - the current state of the art seems to be Jukebox<sup><a href="#ref">[13]</a></sup></li>
      <li>Build a derivative of WaveNet or SampleRNN for a more specific task (e.g. amp modeling), to demonstrate the extensibility of sample-based/waveform domain machine learning models</li>
      <li>Add new synthesizers to DDSP (e.g. FM, AM) to demonstrate that other DSP building blocks can fit into the differentiable framework</li>
    </ul>

    <h1 id="ref">References</h1>
    <ol>
      <li><a href="https://asa.scitation.org/doi/10.1121/1.5133944">Machine learning in acoustics: Theory and applications: The Journal of the Acoustical Society of America: Vol 146, No 5</a></li>
      <li><a href="https://arxiv.org/abs/1609.03499">WaveNet: A Generative Model for Raw Audio - arXiv.org</a></li>
      <li><a href="https://arxiv.org/abs/1612.07837">SampleRNN: An Unconditional End-to-End Neural Audio Generation Model - arXiv.org</a></li>
      <li><a href="https://arxiv.org/abs/1811.06633">Generating Albums with SampleRNN to Imitate Metal, Rock, and Punk Bands - arXiv.org</a></li>
      <li><a href="https://www.mcgill.ca/timbre2018/files/timbre2018/timbre2018_proceedings.pdf">Generating orchestral music by conditioning SampleRNN - PDF, Timbre 2018 proceedings</a></li>
      <li><a href="https://petewarden.com/2018/03/19/the-machine-learning-reproducibility-crisis/">The Machine Learning Reproducability Crisis</a></li>
      <li><a href="https://oddity.ai/blog/best-bang-for-buck-gpu/">The Best Bang for Your Buck Hardware for Deep Learning</a></li>
      <li><a href="https://github.com/Cortexelus/dadabots_sampleRNN/wiki/Installing-Dadabots-SampleRNN-on-Ubuntu">Installing Dadabots SampleRNN on Ubuntu</a></li> 
      <li><a href="https://github.com/soroushmehr/sampleRNN_ICLR2017/issues/7">SampleRNN performance - Issue #7 - GitHub issues</a></li>
      <li><a href="https://openreview.net/pdf/590d904c6dbdcad65fbc7f422b839c4822c07f60.pdf">A Practical Taxonomy of Reproducibility for Machine Learning Research - paper</a></li>
      <li><a href="https://openreview.net/forum?id=B1eYYK5QgX">A Practical Taxonomy of Reproducibility for Machine Learning Research - OpenReview.org review comments</a></li>
      <li><a href="https://thegradient.pub/ai-democratization-in-the-era-of-gpt-3/">AI Democratization in Era of GPT-3</a></li>
      <li><a href="https://openai.com/blog/jukebox/">Jukebox by OpenAI</a></li>
    </ol>
  </body>
</html>
