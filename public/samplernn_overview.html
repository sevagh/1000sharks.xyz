<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="GitLab Pages">
    <title>1000 sharks - an AI music experience</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body id="index">
    <div class="navbar">
      <a href="./">MUMT 618 report</a>
      <a href="./demo.html">1000sharks demo</a>
    </div>

    <h1>SampleRNN overview</h1>
    SampleRNN<sup><a href="#ref">[1]</a></sup> uses recurrent neural networks to find patterns in audio waveforms.
    <br>
    <br>
    Since I can't describe it better than the paper itself, this is their rationale for why RNNs are suitable for audio waveforms:
    <br>
    <br>
    <img src="samplernn_rnn.png" width=750px/>
    <br>
    <br>
    Note the important link to the causality of WaveNet's causal convolutions is that the memory cell uses past memory to compute present samples, or <code>h[t] = H(h[t-1])</code>.
    <br>
    <br>
    SampleRNN is an <b>autoregressive</b> model like WaveNet, meaning that it computes a probabilistic model of what the next most likely sample to generate is, using the following probability distribution:
    <br>
    <br>
    <img src="samplernn_probability.png" width=750px/>
    <br>
    <br>
    A key concept in SampleRNN is the use of a <b>hierarchy of modules</b> operating at different temporal resolutions to capture the temporal patterns present in audio and music. This is related to how WaveNet used <b>dilated convolutions</b> to learn features at wider timescales than immediately consecutive samples.
    <br>
    <br>
    The authors of SampleRNN also agree with WaveNet that discretizing the inputs gives better results than operating on the float samples directly:
    <blockquote>
We use a Softmax because we found that better results were obtained by discretizing the audio signals (also see van den Oord et al. (2016)) and outputting a Multinoulli distribution rather than using a Gaussian or Gaussian mixture to represent the conditional density of the original real-valued signal.
    </blockquote>
    The reference implementation by the original authors of the paper, <a href="https://github.com/ibab/tensorflow-wavene://github.com/soroushmehr/sampleRNN_ICLR2017">sampleRNN_ICLR2017</a>, is less readable (to me) than the <a href="https://github.com/rncm-prism/prism-samplernn">PRiSM-SampleRNN</a> implementation, so I'll use the second for the code dissection.

    <h2>2-tier vs. 3-tier SampleRNN</h2>
    Something to note is that various sources mention that 2-tier SampleRNN creates better music than 3-tier SampleRNN:
    <ul>
	    <li> Dadabots<sup><a href="#ref">https://github.com/Cortexelus/dadabots_sampleRNN#training-samplernn-3-tier</a></sup>:
		    <blockquote>There's also a 3-tier option, but we initially had better results with 2-tier, so we don't use 3-tier. It doesn't have the modifications we made to 2-tier.</blockquote>
	    </li>
	    <li> SampleRNN paper reviewer comments<sup><a href="#ref">https://openreview.net/forum?id=SkxKPDv5xl</a></sup>:
		    <blockquote>Table 1 and Figure 4 also show the 2-tier SampleRNN outperforming the 3-tier model in terms of likelihood and human rating respectively, which is very counterintuitive as one would expect longer-range temporal correlations to be even more relevant for music than for speech. This is not discussed at all, I think it would be useful to comment on why this could be happening.</blockquote>
		 Author's reply:
		 <blockquote>
			 "Why 2-tier is outperforming the 3-tier model for music?"
			 - We did not expect that, but for any dataset and architecture structure, there is an optimal depth. Considering that this is a deep RNN (which introduces a form of recurrent depth, here very large) and the hypothesis that it is difficult to train such architectures in the first place, it is possible that alternative training procedures could yield better results with a deeper model.
		 </blockquote>
	    </li>
    </ul>
    <br>
    Recall that in the SampleRNN paper, the multiple tiers of the RNN determined the learning of audio patterns at different temporal scales. This is reflected in the following diagram from the paper:
    <figure>
    <img src="8_samplernn.png" width=700px/>
    <figcaption>Higher RNN tiers map to wider temporal scales <sup><a href="#ref">[3]</a></sup></figcaption>
    </figure>
    <br>
    The lowest temporal scale (consecutive samples) represents very low-level audio features (e.g. timbre), while higher scales can (hypothetically) go as far as representing repeating choruses or verses minutes apart. As such, it's interesting to note that 2-tiers, or only two temporal scales of learning, performed better than 3-tier, which should hypothetically be enforcing even longer-scale temporal patterns (and music has temporal patterns as coarse as minutes apart, e.g. a repeating chorus).
    <br>
    <br>
    However, as the author says, 2-tier SampleRNN may have a depth that makes it more optimal considering the training architecture of SampleRNN (or in other words, there needs to be an analysis of alternative training architectures to make 3-tier beat 2-tier).

    <h2>SampleRNN configuration</h2>
    I'll summarize the available SampleRNN hyperparameters and other customizeable steps compared across the original 2017 ICLR implementation, the Dadabots fork, the PRiSM fork which I use throughout the rest of this page, and finally my own modifications to the PRiSM parameters after experiment 0:
    <br>
    <br>
    <table>
      <tr>
        <th></th>
        <th>Original</th>
        <th>Dadabots</th>
        <th>PRiSM</th>
        <th>Descr</th>
      </tr>
      <tr>
        <th>RNN layers</th>
        <td>4</td>
        <td>5</td>
        <td>4</td>
        <td>Quality of results (dadabots note that 5 learns music better than 4)</td>
      </tr>
      <tr>
        <th>Tiers</th>
        <td>2 or 3</td>
        <td>2 or 3 (2 recommended for good music)</td>
        <td>3</td>
        <td>Tiers of RNN (more = wider temporal timescale, but...*)</td>
      </tr>
      <tr>
        <th>Frame sizes (corresponds to tiers)</th>
        <td>16</td>
        <td>16</td>
        <td>16,64</td>
        <td>Samples apart between low and high timescales</td>
      </tr>
      <tr>
        <th>Sample rate</th>
        <td>16000 (fixed)</td>
        <td>16000</td>
        <td>16000</td>
        <td>Sample rate of training/generating waveform (lower = faster learning, better able to learn long-timescale patterns)</td>
      </tr>
      <tr>
        <th>Training input</th>
        <td>No details</td>
        <td>Chop albums into 8s + 1s overlap</td>
        <td>Chop albums into 8s + 1s overlap</td>
        <td>Suggestions on how to prepare training data</td>
      </tr>
      <tr>
        <th>Epochs</th>
        <td>Not customizeable</td>
        <td>Not customizeable</td>
        <td>100</td>
        <td>Entire cycles of training on the same data (more = possibly better learning, but not necessarily)</td>
      </tr>
    </table> 
    <br>
    The frame sizes 16, 64 correspond to the additional tiers of SampleRNN (the first tier is always n = 1, or consecutive samples). A 2-tier SampleRNN architecture has a wider temporal scale of learning at 16 frames, while a 3-tier SampleRNN architecture learns at 16 and 64 frames.

    <h3>Preprocessing</h3>
    The preprocessing uses the same mu-law encoding as the WaveNet but without the one-hot encoding. The 256-bit quantized integer is used directly instead of converting it into a 256-length vector.
    <h3>Training layers - stack of hierarchical RNN modules</h3>
    <h3>Loss function</h3>
    The loss function and training parameter optimization code is very similar to the WaveNet code, down to the optimizer factory. From <code>train.py</code>:
    <pre>
    def create_adam_optimizer(learning_rate, momentum):
        return tf.optimizers.Adam(learning_rate=learning_rate,
                                  epsilon=1e-4)
    
    def create_sgd_optimizer(learning_rate, momentum):
        return tf.optimizers.SGD(learning_rate=learning_rate,
                                 momentum=momentum)
    
    def create_rmsprop_optimizer(learning_rate, momentum):
        return tf.optimizers.RMSprop(learning_rate=learning_rate,
                                     momentum=momentum,
                                     epsilon=1e-5)
    
    optimizer_factory = {'adam': create_adam_optimizer,
                     'sgd': create_sgd_optimizer,
                     'rmsprop': create_rmsprop_optimizer}

    # Optimizer
    opt = optimizer_factory[args.optimizer](
        learning_rate=args.learning_rate,
        momentum=args.momentum,
    )

    # Compile the model
    compute_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')
    model.compile(optimizer=opt, loss=compute_loss, metrics=[train_accuracy])
    </pre>
    These are passed into the SampleRNN model code (<code>samplernn/model.py</code>):
    <pre>
    def train_step(self, data):
        (x, y) = data
        with tf.GradientTape() as tape:
            raw_output = self(x, training=True)
            prediction = tf.reshape(raw_output, [-1, self.q_levels])
            target = tf.reshape(y, [-1])
            loss = self.compiled_loss(
                target,
                prediction,
                regularization_losses=self.losses)
        grads = tape.gradient(loss, self.trainable_variables)
        grads, _ = tf.clip_by_global_norm(grads, 5.0)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
        self.compiled_metrics.update_state(target, prediction)
        return {metric.name: metric.result() for metric in self.metrics}
    </pre>
    Like with WaveNet, a predicted waveform is produced from the model during training and then compared to the input waveforms to compute the loss. The actual prediction is done with <code>self(x, training=True)</code>, which in Python would be implemented by the object's <code>call()</code> function:
    <pre>
    def call(self, inputs, training=True, temperature=1.0):
       # UPPER TIER
       big_frame_outputs = self.big_frame_rnn(
           tf.cast(inputs, tf.float32)[:, : -self.big_frame_size, :]
       )
       # MIDDLE TIER
       frame_outputs = self.frame_rnn(
           tf.cast(inputs, tf.float32)[:, self.big_frame_size-self.frame_size : -self.frame_size, :],
           conditioning_frames=big_frame_outputs,
       )
       # LOWER TIER (SAMPLES)
       sample_output = self.sample_mlp(
           inputs[:, self.big_frame_size - self.frame_size : -1, :],
           conditioning_frames=frame_outputs,
       )
       return sample_output
    </pre>
    The values are <b>16-sample frames</b> for the middle tier, and <b>64-sample frames</b> for the upper tier.
    Here we see a key distinction between SampleRNN and WaveNet. WaveNet uses the weights of the dilated convolution network to predict samples with knowledge of different temporal scales built in. SampleRNN is using patterns learned at broad temporal scales to condition the lower temporal scales - this means that SampleRNN's <b>choice</b> of high-level/long-term temporal feature <b>feeds into the subsequent choices</b> for the low-level temporal feature predictions.
    <br>
    <br>
    The optimizers are the same. The loss function is in fact the same as WaveNet, except that WaveNet used TensorFlow 1's <code>softmax_cross_entropy_with_logits</code> function<sup><a href="#ref">oldapi_loss</a></sup>, while SampleRNN uses a slightly different API<sup><a href="#ref">newapi_loss</a></sup>,<code>SparseCateoricalCrossentropy</code>. The difference is explained simply that if your data is one-hot encoded (i.e. 256-bit mu-law integers expanded into a vector of 256 0s or 1s, like WaveNet), you would use the softmax cross entropy function, whereas if they're integers (like SampleRNN), you would use a sparse softmax cross entropy function.
    <br>
    <br>
    We can also see the input data is batched as in WaveNet, indicating similar use of mini-batch iterative training (aka mini-batch Stochastic Gradient Descent):
    <pre>
    initial_epoch = get_initial_epoch(resume_from)
    dataset = get_dataset(args.data_dir, args.num_epochs-initial_epoch, args.batch_size, seq_len, overlap)

    # Dataset iterator
    def train_iter():
        for batch in dataset:
            num_samps = len(batch[0])
            for i in range(overlap, num_samps, seq_len):
                x = quantize(batch[:, i-overlap : i+seq_len], q_type, q_levels)
                y = x[:, overlap : overlap+seq_len]
                yield (x, y)

    callbacks = [
        TrainingStepCallback(
            model = model,
            num_epochs = args.num_epochs,
            steps_per_epoch = steps_per_epoch,
            steps_per_batch = steps_per_batch,
        ModelCheckpointCallback(
            monitor = 'loss',
            save_weights_only = True,
            save_best_only = args.checkpoint_policy.lower()=='best',
            save_freq = args.checkpoint_every * steps_per_epoch),
        tf.keras.callbacks.EarlyStopping(
            monitor = 'loss',
            patience = args.early_stopping_patience),
    ]
    </pre>
    The above code is the equivalent of the training loop of WaveNet, where the SampleRNN model exposes its trainable variables and the Tensorflow library is leveraged to use the loss function above to train the model.
    <br>
    <br>
    Note a detail of SampleRNN is that they're using the keras EarlyStopping<sup><a href="#ref">keras earlystop</a></sup> which stops training if it predicts that the model is not improving enough to be worthwhile. In WaveNet, the training proceeds for as many steps as the user requested.
    <h3>Generating audio</h3>
    <h1 id="ref">References</h2>
    <ol>
      <li><a href="https://arxiv.org/abs/1612.07837">SampleRNN: An Unconditional End-to-End Neural Audio Generation Model - arXiv.org</a></li>
    </ol>
  </body>
</html>

